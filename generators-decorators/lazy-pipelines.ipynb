{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75498774-9207-44cf-a5ee-e76ddfdedd3a",
   "metadata": {},
   "source": [
    "# Generators and Lazy Pipelines\n",
    "\n",
    "- You can chain generator functions to form multi-stage data pipelines that process items one at a time.  \n",
    "- No intermediate lists are built, so memory stays low even for very large streams.  \n",
    "- Each generator only holds its own minimal state and passes items downstream on demand.  \n",
    "\n",
    "## Memory Efficiency\n",
    "\n",
    "- Lazy iterables maintain only minimal state (like start, stop, step) regardless of total length.  \n",
    "- Eager collections (lists, tuples) grow in memory usage as you add items.  \n",
    "- Use `sys.getsizeof()` to inspect the in-memory size of objects themselves (not their contents).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6da1ba08-a588-4f4b-9c07-e444466f810d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User user77 logged out\n",
      "User user75 logged out\n",
      "User user1 logged in\n",
      "User user85 logged in\n",
      "Generator object sizes (in bytes): 232 240 224\n"
     ]
    }
   ],
   "source": [
    "# 1. Ingest the log lines from a file.\n",
    "# 2.  Filter the log lines based on either level or message substring.\n",
    "#3 . Extract and return only the message attributes from the filtered log lines.\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "file_path = Path.cwd()/\"generators-decorators/large_logs.txt\"\n",
    "\"\"\" script_dir = os.path.dirname(__file__)           # Folder where the script is located\n",
    "file_path = os.path.join(script_dir, \"large_logs.txt\") \"\"\"\n",
    "\n",
    "# print(os.getcwd())\n",
    "\n",
    "def read_logs(filepath):\n",
    "    \"\"\" Reads the  contents of a log file line by line and yields each line as a JSON object. \n",
    "    Args:\n",
    "        filepath (str): The path to the log file.\n",
    "    Yields:\n",
    "        generator(dict(str)): A JSON object representing a log line.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue # Skip empty lines\n",
    "            yield json.loads(line)\n",
    "\n",
    "def filter_logs(logs, level=None, mesage_substring=None):\n",
    "    \"\"\" Filters log entries based on log level or message substring.\n",
    "    Args:\n",
    "        logs (iterable(dict)): Iterable containing the logs to be filtered\n",
    "        level (str): The log level to filter by (e.g., \"ERROR\", \"INFO\"). Defaults to None.\n",
    "        message_substring (str): A substring to search for in the log messages. Defaults to None.\n",
    "    Yields:\n",
    "        generator(dict(str)): A filtered log entry as a JSON object.\n",
    "    \"\"\"\n",
    "    for log in logs:\n",
    "        if (\n",
    "            level is not None \n",
    "            and log.get('level', \"\").lower() != level.lower()\n",
    "            ):\n",
    "            continue\n",
    "        if (\n",
    "            mesage_substring is not None \n",
    "            and mesage_substring.lower() not in log.get('message', '').lower()\n",
    "            ):\n",
    "            continue    \n",
    "        yield log    \n",
    "\n",
    "def extract_field(logs, field = \"message\"):\n",
    "    \"\"\" Extracts a specific field from each log entry.\n",
    "    Args:\n",
    "        logs (iterable(dict)): Iterable containing the logs to extract the field from.\n",
    "        field (str): The field to extract from each log entry. Defaults to \"message\".       \n",
    "    Yields:\n",
    "        generator(str): The extracted field value from each log entry.\n",
    "    \"\"\"\n",
    "    for log in logs:\n",
    "        yield log.get(field,\"\").strip()    \n",
    "\n",
    "\n",
    "def get_first_n(logs, n=5):\n",
    "    \"\"\" Retrieves the first n log entries from an iterable.\n",
    "    Args:\n",
    "        logs (iterable(dict)): Iterable containing the logs to retrieve from.\n",
    "        n (int): The number of log entries to retrieve. Defaults to 5.  \n",
    "    Yields:\n",
    "        generator(dict(str)): The first n log entries as JSON objects.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for log in logs:\n",
    "        if count >= n:\n",
    "            break\n",
    "        yield log\n",
    "        count += 1\n",
    "\n",
    "# print(listdir())\n",
    "logs_gen = read_logs(file_path)\n",
    "filter_gen = filter_logs(logs_gen, \"INFO\", \"user\")\n",
    "#next(logs_gen)  # Advance the generator to the first log entry\n",
    "#next(filter_gen)  # Advance the generator to the first filtered log entry\n",
    "extract_gen = extract_field(filter_gen, \"message\")\n",
    "for log in get_first_n(extract_gen, 4):\n",
    "    print(log)\n",
    "\n",
    "print(\"Generator object sizes (in bytes):\",\n",
    "      sys.getsizeof(logs_gen),\n",
    "      sys.getsizeof(filter_gen),\n",
    "      sys.getsizeof(extract_gen)\n",
    "      )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
