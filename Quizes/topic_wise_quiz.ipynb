{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75537858",
   "metadata": {},
   "source": [
    "## Function and Iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a950fd6c",
   "metadata": {},
   "source": [
    "A function is defined to deploy an application, with default values for its version and namespace. How can you call this function to deploy the app in the 'staging' namespace while using the default version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0798bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying auth-service v1.0.0 to staging namespace.\n"
     ]
    }
   ],
   "source": [
    "def deploy_app(app_name, version='1.0.0', namespace='default'):\n",
    "    print(f\"Deploying {app_name} v{version} to {namespace} namespace.\")\n",
    "\n",
    "deploy_app('auth-service', namespace=\"staging\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4624acfc",
   "metadata": {},
   "source": [
    "A script is mapping server names to their respective datacenter locations. What will be the output of this code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c36b239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'web-1' is in 'us-east'\n",
      "'db-1' is in 'us-west'\n",
      "'app-1' is in 'eu-central'\n"
     ]
    }
   ],
   "source": [
    "servers = ['web-1', 'db-1', 'app-1', 'cache-1']\n",
    "locations = ['us-east', 'us-west', 'eu-central']\n",
    " \n",
    "for server, loc in zip(servers, locations):\n",
    "    print(f\"'{server}' is in '{loc}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4408eb46",
   "metadata": {},
   "source": [
    "A script needs to display a numbered list of pending software updates. Which code snippet correctly uses enumerate to produce a numbered list starting with \"1.\" for the first item, and incrementing the number by one for each item of the following list?\n",
    "\n",
    "\n",
    "\n",
    "updates = ['kernel-patch', 'openssl-fix', 'python-update']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9f8c7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: kernel-patch\n",
      "2: openssl-fix\n",
      "3: python-update\n"
     ]
    }
   ],
   "source": [
    "updates = ['kernel-patch', 'openssl-fix', 'python-update']\n",
    "for i, update in enumerate(updates, start=1):\n",
    "    print(f\"{i}: {update}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f17165e",
   "metadata": {},
   "source": [
    "The function below is intended to report on a list of tasks without changing the original list. However, after the function call, the pending_deploys list is empty. What is the cause of this bug, and how should it be fixed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0bfff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_tasks(task_list):\n",
    "    print(\"Tasks to process:\")\n",
    "    while task_list:\n",
    "        task = task_list.pop(0)\n",
    "        print(f\"- {task}\")\n",
    "    print(\"Report complete.\")\n",
    " \n",
    "pending_deploys = ['deploy-web', 'deploy-db', 'deploy-cache']\n",
    "report_tasks(pending_deploys)\n",
    "# This next line unexpectedly prints: \"Tasks remaining: []\"\n",
    "print(f\"Tasks remaining: {pending_deploys}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80329686",
   "metadata": {},
   "source": [
    "The .pop(0) method is modifying the list passed to it. The fix is to create a copy of the list for processing, by changing the first line of the function to task_list = task_list.copy()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89365684",
   "metadata": {},
   "source": [
    "## OOP in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec4070c",
   "metadata": {},
   "source": [
    "When defining a Python class to represent a server, what is the primary role of the __init__ method?\n",
    "\n",
    "Answer: It is a method that is automatically called when an instance of the class is created, used to initialize the instance's unique attributes(for example, self.hostname = \"web01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189f963b",
   "metadata": {},
   "source": [
    "A WebServer class inherits from a generic Server class. Both classes have a shutdown() method. How can the WebServer's shutdown() method first perform its own specific actions and then call the generic shutdown logic from the Server class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4def8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebServer(Server):\n",
    "    def shutdown(self):\n",
    "        print(\"Shutting down web server.\")\n",
    "        super().shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f2038",
   "metadata": {},
   "source": [
    "What is the fundamental difference between an instance attribute (e.g., self.hostname) and a class attribute?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9818bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server:\n",
    "    # Class attribute\n",
    "    location = \"US-EAST-1\"\n",
    " \n",
    "    def __init__(self, hostname):\n",
    "        # Instance attribute\n",
    "        self.hostname = hostname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbcd2f2",
   "metadata": {},
   "source": [
    "A class attribute is shared by all instances of the class, while an instance attribute is unique to each specific object.\n",
    "All objects created from the Server class will share the single location attribute. However, each object will have its own distinct hostname attribute, set when the object is created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dc7dd9",
   "metadata": {},
   "source": [
    "Two instances of a User class are created. What will be the output of the final print statement?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2aba97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "admin: True, guest: False\n"
     ]
    }
   ],
   "source": [
    "class User:\n",
    "    def __init__(self, username):\n",
    "        self.username = username\n",
    "        self.is_active = False\n",
    " \n",
    "    def activate(self):\n",
    "        self.is_active = True\n",
    " \n",
    "user1 = User(\"admin\")\n",
    "user2 = User(\"guest\")\n",
    "user1.activate()\n",
    " \n",
    "print(f\"{user1.username}: {user1.is_active}, {user2.username}: {user2.is_active}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996ebdee",
   "metadata": {},
   "source": [
    "An engineer wrote a class to monitor services. When they create an instance and check its status, they get an AttributeError. What is the cause of this bug?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4703ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ServiceMonitor:\n",
    "    def __init__(self, service_name):\n",
    "        name = service_name\n",
    "        is_alive = False\n",
    " \n",
    "    def check_status(self):\n",
    "        print(f\"Checking status of {self.name}...\")\n",
    "        self.is_alive = True\n",
    "        return self.is_alive\n",
    " \n",
    "monitor = ServiceMonitor(\"database\")\n",
    "monitor.check_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6045ebb0",
   "metadata": {},
   "source": [
    "The __init__ method created local variables name and is_alive instead of instance attributes. The fix is to use self.name= service_name and self.is_alive=False."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb98e5f1",
   "metadata": {},
   "source": [
    "Consider the following class definition. What is the output of the code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ff86f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "web-01 on Xen, db-01 on Xen\n"
     ]
    }
   ],
   "source": [
    "class VM:\n",
    "    # Class attribute\n",
    "    hypervisor = \"KVM\"\n",
    " \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    " \n",
    "vm1 = VM(\"web-01\")\n",
    "vm2 = VM(\"db-01\")\n",
    " \n",
    "# The hypervisor for all VMs is upgraded\n",
    "VM.hypervisor = \"Xen\"\n",
    " \n",
    "print(f\"{vm1.name} on {vm1.hypervisor}, {vm2.name} on {vm2.hypervisor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c037448",
   "metadata": {},
   "source": [
    "A child class DatabaseServer is created from a parent Server class. What is the correct way to initialize both the parent's attributes (hostname) and the child's specific attribute (db_engine)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d914dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseServer(Server):\n",
    "    def __init__(self, hostname, db_engine):\n",
    "        super().__init__(hostname)\n",
    "        self.db_engine = db_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ea333",
   "metadata": {},
   "source": [
    "## Working with Flexible arguements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7276005",
   "metadata": {},
   "source": [
    "A function is designed to gather system metrics, where some metrics are required and others are optional. What is the value of the extra_metrics dictionary inside the function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7529cb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_metrics(hostname, *base_metrics, **extra_metrics):\n",
    "    # What is the value of extra_metrics?\n",
    "    pass\n",
    " \n",
    "gather_metrics(\"web01\", \"cpu_usage\", \"mem_usage\", disk_io=45.5, network_traffic=\"200MB/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e3b350",
   "metadata": {},
   "source": [
    "{'disk_io': 45.5, 'network_traffic' : '200MB/s'} \n",
    "**extra_mtrics collects all keyword arguements  that are not explicitly defined as parameters. In this call, disk_io and network_traffic are passed as keyword arguements and are therefore collected into the extra_metrics dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7fe341",
   "metadata": {},
   "source": [
    "Q: A function is defined to accept a variable number of file paths to process. What is the value and data type of the paths variable inside the function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6367236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(*paths):\n",
    "    # What is `paths` here?\n",
    " \n",
    "process_files(\"/etc/hosts\", \"/etc/nginx/nginx.conf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9e35cd",
   "metadata": {},
   "source": [
    "A: It is a tuple contatining (\"/etc/hosts\", \"/etc/nginx/nginx.conf\"). The *syntax in a function definition gathers all provided arguements into a single tuple paths to maintain the order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a65422",
   "metadata": {},
   "source": [
    "Q: A function needs to be called with configuration settings stored in a dictionary. What is the correct syntax to unpack the db_config dictionary into keyword arguments for the connect_to_db function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a144dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def connect_to_db(host, port, user, password):\n",
    "    print(f\"Connecting to {host}:{port} as {user}...\")\n",
    " \n",
    "db_config = {'host': 'db.prod', 'port': 5432, 'user': 'admin', 'password': '123'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a406c2",
   "metadata": {},
   "source": [
    "A: connect_to_db(**db_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2d30d3",
   "metadata": {},
   "source": [
    "Q: An engineer wants to create a flexible function that takes a required command and optional keyword arguments for flags. The code below, however, raises a TypeError. What is the cause of the error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47330165",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_command(command, **options):\n",
    "    option_str = \" \".join([f\"--{k}={v}\" for k, v in options.items()])\n",
    "    print(f\"Executing: {command} {option_str}\")\n",
    " \n",
    "run_command(\"deploy\", \"app-server\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9574a4af",
   "metadata": {},
   "source": [
    "The string \"app-server\" is passed as a positional arguement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb5db3e",
   "metadata": {},
   "source": [
    "Q: In what order must the different types of parameters appear in a Python function definition?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e728f36",
   "metadata": {},
   "source": [
    "A: Standard positional arguements, *args, keyowrd-only arguements, **kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06e24d7",
   "metadata": {},
   "source": [
    "Q: A function is called by unpacking a list of values. What will be the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150b45c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_health(host, port, timeout):\n",
    "    print(f\"Checking {host} on port {port} with a {timeout}s timeout.\")\n",
    " \n",
    "params = [\"api.service.local\", 443, 10, \"extra_value\"]\n",
    "check_health(*params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050af0cb",
   "metadata": {},
   "source": [
    "A: A TypeError is raised because the params list contains more items than the function has parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5259141",
   "metadata": {},
   "source": [
    "Q: A function is designed to merge multiple configuration dictionaries. The first dictionary provides base settings, and subsequent dictionaries provide overrides. What will be the final value of final_config?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7926241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_configs(base_config, *override_configs):\n",
    "    merged = base_config.copy()\n",
    "    for config in override_configs:\n",
    "        merged.update(config)\n",
    "    return merged\n",
    " \n",
    "config1 = {'user': 'admin', 'retries': 3}\n",
    "config2 = {'retries': 5, 'timeout': 30}\n",
    "config3 = {'timeout': 60, 'loglevel': 'debug'}\n",
    " \n",
    "final_config = merge_configs(config1, config2, config3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fb3a395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user': 'admin', 'retries': 5, 'timeout': 60, 'loglevel': 'debug'}\n"
     ]
    }
   ],
   "source": [
    "print(final_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721816c2",
   "metadata": {},
   "source": [
    "## Generator Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c704c9aa",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "Consider the following Python code. What will be printed to the console when it is executed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8554d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reports(count):\n",
    "    print(\"Initializing report generator...\")\n",
    "    for i in range(count):\n",
    "        yield f\"Report #{i+1}\"\n",
    "    print(\"All reports generated.\")\n",
    " \n",
    "reports_generator = generate_reports(3)\n",
    "print(\"Generator created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c6360d",
   "metadata": {},
   "source": [
    "Generator created\n",
    " The body of a generator function does not execute when the function is called. Instead, a generator object is created and returned immediately. The code inside the generate_reports function (including the print statements) will only run when the generator is iterated over (e.g., with next() or a for loop).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad44eff4",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "What is the output of the following code snippet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a607a91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yielding 10\n",
      "Yielding 15\n",
      "Final value: 15\n"
     ]
    }
   ],
   "source": [
    "def sequence_generator():\n",
    "    value = 10\n",
    "    print(f\"Yielding {value}\")\n",
    "    yield value\n",
    " \n",
    "    value += 5\n",
    "    print(f\"Yielding {value}\")\n",
    "    yield value\n",
    " \n",
    "    value *= 2\n",
    "    print(f\"Yielding {value}\")\n",
    "    yield value\n",
    " \n",
    "gen = sequence_generator()\n",
    "next(gen)\n",
    "val = next(gen)\n",
    "print(f\"Final value: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b6d4f1",
   "metadata": {},
   "source": [
    "The first next(gen) call starts the generator; it prints \"Yielding 10\" and yields 10, then pauses. The second next(gen) call resumes execution; value is incremented to 15, \"Yielding 15\" is printed, and 15 is yielded and assigned to val. The program then prints the final line. The code for the third yield is never reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78ac22e",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "A developer wrote the following script to create pairs of network zones. They expected to see all possible pairs (e.g., \"Web:DB\", \"Web:API\", \"DB:Web\", etc.), but the output was not what they expected. What is the bug, and how should it be fixed in the most efficient way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad1a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zones():\n",
    "    yield \"Web\"\n",
    "    yield \"DB\"\n",
    "    yield \"API\"\n",
    " \n",
    "zones_gen = get_zones()\n",
    " \n",
    "for zone1 in zones_gen:\n",
    "    for zone2 in zones_gen:\n",
    "        print(f\"{zone1}:{zone2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59362f93",
   "metadata": {},
   "source": [
    "The bug is that the zones_gen generator is exhausted by the inner loop on its first run. The fix is to get a fresh generator for the inner loop by calling the factory function again: for zone2 in get_zones():.\n",
    "\n",
    "This is the correct answer. The inner loop for zone2 in zones_gen: completely consumes the generator. When the outer loop starts its second iteration (e.g., with zone1 as \"DB\"), the zones_gen is already empty (exhausted), so the inner loop does not run again. Creating a new generator for each loop (for zone1 in get_zones(): and for zone2 in get_zones():) ensures each loop works with a fresh, independent iterator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59933e9",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "What will be the output of this Python script, which attempts to iterate over a generator twice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64b529c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First pass:\n",
      "- sensor-A\n",
      "- sensor-B\n",
      "\n",
      "Second pass:\n"
     ]
    }
   ],
   "source": [
    "def device_ids():\n",
    "    yield \"sensor-A\"\n",
    "    yield \"sensor-B\"\n",
    " \n",
    "ids = device_ids()\n",
    " \n",
    "print(\"First pass:\")\n",
    "for device_id in ids:\n",
    "    print(f\"- {device_id}\")\n",
    " \n",
    "print(\"\\nSecond pass:\")\n",
    "for device_id in ids:\n",
    "    print(f\"- {device_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ac340c",
   "metadata": {},
   "source": [
    "This is the correct answer. The first for loop consumes all the values from the ids generator, exhausting it. When the second for loop attempts to iterate over the same exhausted generator, the generator immediately signals it has no more items (by raising StopIteration internally, which the for loop handles), so the loop's body never executes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a99e6a",
   "metadata": {},
   "source": [
    "Question 5:\n",
    "A generator function is defined to yield a sequence of status messages. What is the exact output when the following code is run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c5134d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: PENDING\n",
      "2: IN_PROGRESS\n",
      "Final state: COMPLETED\n",
      "3: Deployment finished.\n"
     ]
    }
   ],
   "source": [
    "def deployment_status():\n",
    "    status = \"PENDING\"\n",
    "    yield status\n",
    " \n",
    "    status = \"IN_PROGRESS\"\n",
    "    yield status\n",
    " \n",
    "    status = \"COMPLETED\"\n",
    "    print(f\"Final state: {status}\")\n",
    " \n",
    "d_status = deployment_status()\n",
    "print(f\"1: {next(d_status)}\")\n",
    "print(f\"2: {next(d_status)}\")\n",
    "try:\n",
    "    next(d_status)\n",
    "except StopIteration:\n",
    "    print(\"3: Deployment finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dbec55",
   "metadata": {},
   "source": [
    "This is the correct answer. The first two next() calls consume the first two yielded values. The third next() call resumes the generator. It executes the print(f\"Final state: {status}\") line, then the function finishes, which causes a StopIteration to be raised. The except block catches this and prints its message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff4f9f1",
   "metadata": {},
   "source": [
    "## Building Lazy pipelines with Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9bdceb",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "A DevOps engineer needs to process a 50 GB log file to count the number of lines containing the word \"FATAL\". The engineer's machine has only 8 GB of RAM. Which of the following statements best explains the primary advantage of using a lazy generator pipeline for this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef62bb8b",
   "metadata": {},
   "source": [
    "Answer: \n",
    "The pipeline avoids loading the entire 50 GB file into memory at once. It processes the file line-by-line, keeping memory usage minimal and constant regardless of the file size.\n",
    "\n",
    "Correct! This is the core benefit of lazy pipelines. By reading and processing the file one line at a time, the memory footprint remains extremely low. The entire 50 GB file is never held in RAM, making it possible to process on a machine with much less memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf40f8e",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "You are given a pipeline of generators to process a list of raw transaction data. What will be the final output of the following script?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a511ffc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying fees...\n",
      "Filtering...\n",
      "Parsing...\n",
      "[180.0, 135.0]\n"
     ]
    }
   ],
   "source": [
    "def parse_transactions(data):\n",
    "    print(\"Parsing...\")\n",
    "    for item in data:\n",
    "        parts = item.split(':')\n",
    "        yield (parts[0], int(parts[1]))\n",
    " \n",
    "def filter_high_value(transactions, min_value=100):\n",
    "    print(\"Filtering...\")\n",
    "    for _, amount in transactions:\n",
    "        if amount >= min_value:\n",
    "            yield amount\n",
    " \n",
    "def apply_fees(amounts, fee_percent=10):\n",
    "    print(\"Applying fees...\")\n",
    "    for amount in amounts:\n",
    "        yield amount - (amount * fee_percent / 100)\n",
    " \n",
    "raw_data = [\"TX01:50\", \"TX02:200\", \"TX03:150\", \"TX04:90\"]\n",
    " \n",
    "pipeline = apply_fees(filter_high_value(parse_transactions(raw_data)))\n",
    " \n",
    "result = list(pipeline)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc235860",
   "metadata": {},
   "source": [
    "Correct. The list() constructor pulls items one by one through the entire pipeline. The print statements inside the generators are executed only when the pipeline is first consumed at each stage. parse_transactions yields ('TX02', 200), which filter_high_value yields as 200, and apply_fees yields as 180.0. This repeats for 150, which becomes 135.0. The other items are filtered out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb19562",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "The following script is intended to first iterate through all \"login\" events to perform an action (simulated by the print statement) and then, in a separate step, count the total number of these events. However, the script contains a bug and reports an incorrect count of 0. What is the fundamental cause of this bug, and what is the most suitable solution to fix it while preserving the two-step logic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5245cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_login_events(all_events):\n",
    "    for event in all_events:\n",
    "        if event.get(\"type\") == \"login\":\n",
    "            print(f\"Found login for user: {event['user_id']}\")\n",
    "            yield event\n",
    " \n",
    "events = [\n",
    "    {\"user_id\": 101, \"type\": \"login\"},\n",
    "    {\"user_id\": 102, \"type\": \"page_view\"},\n",
    "    {\"user_id\": 103, \"type\": \"login\"},\n",
    "    {\"user_id\": 104, \"type\": \"logout\"},\n",
    "    {\"user_id\": 105, \"type\": \"login\"},\n",
    "]\n",
    " \n",
    "login_stream = get_login_events(events)\n",
    " \n",
    "# Step 1: Perform an action on each login event\n",
    "for _ in login_stream:\n",
    "    pass\n",
    " \n",
    "# Step 2: Count the total number of login events\n",
    "count = len(list(login_stream))\n",
    "print(f\"Total logins: {count}\") # Buggy output: Total logins: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43724987",
   "metadata": {},
   "source": [
    "Cause: The login_stream generator is exhausted after the first for loop. A generator can only be iterated over once.\n",
    "Solution: Convert the generator to a list at the beginning and perform both operations on that reusable list.\n",
    "\n",
    "\n",
    "\n",
    "Correct. This choice accurately identifies generator exhaustion as the root cause. The proposed solution is the most practical fix for scenarios requiring multiple passes over the same dataset. It eagerly evaluates the generator into a list, which can then be used repeatedly without issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d5523",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "Consider this two-stage pipeline designed to process a sequence of numbers. What is the exact sequence of printed output when this script is executed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02433e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Getting first item ---\n",
      "DOUBLER: Starting\n",
      "SOURCE: Starting\n",
      "SOURCE: Yielding 0\n",
      "DOUBLER: Processing 0\n",
      "Result: 0\n",
      "--- Getting second item ---\n",
      "SOURCE: Yielding 1\n",
      "DOUBLER: Processing 1\n",
      "Result: 2\n"
     ]
    }
   ],
   "source": [
    "def number_source(n):\n",
    "    print(\"SOURCE: Starting\")\n",
    "    for i in range(n):\n",
    "        print(f\"SOURCE: Yielding {i}\")\n",
    "        yield i\n",
    "    print(\"SOURCE: Finished\")\n",
    " \n",
    "def doubler(items):\n",
    "    print(\"DOUBLER: Starting\")\n",
    "    for item in items:\n",
    "        print(f\"DOUBLER: Processing {item}\")\n",
    "        yield item * 2\n",
    "    print(\"DOUBLER: Finished\")\n",
    " \n",
    "pipeline = doubler(number_source(2))\n",
    "print(\"--- Getting first item ---\")\n",
    "print(f\"Result: {next(pipeline)}\")\n",
    "print(\"--- Getting second item ---\")\n",
    "print(f\"Result: {next(pipeline)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d2f2a2",
   "metadata": {},
   "source": [
    "Correct. This output accurately traces the lazy, \"pull-based\" execution. The call to next(pipeline) pulls a value from doubler, which in turn pulls a value from number_source. The first item 0 is yielded from source, processed by doubler, and returned as 0. The second call to next resumes the process, pulling 1 from source, which is processed by doubler and returned as 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3335fe42",
   "metadata": {},
   "source": [
    "Question 5:\n",
    "The following script is intended to create a lazy pipeline that identifies IP addresses that transferred more than 1000 bytes and then formats them for a report. However, the filter_heavy_hitters function breaks the lazy evaluation model. How should the filter_heavy_hitters function be rewritten to ensure the entire pipeline is lazy and memory-efficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf7f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_logs(log_lines):\n",
    "    for line in log_lines:\n",
    "        parts = line.split()\n",
    "        if len(parts) == 2:\n",
    "            yield (parts[0], int(parts[1]))\n",
    " \n",
    "def filter_heavy_hitters(records):\n",
    "    # This function is not lazy\n",
    "    results = []\n",
    "    for ip, byte_count in records:\n",
    "        if byte_count > 1000:\n",
    "            results.append(ip)\n",
    "    return results # Eagerly returns a list\n",
    " \n",
    "def format_for_report(ip_addresses):\n",
    "    for ip in ip_addresses:\n",
    "        yield f\"ALERT: High traffic from {ip}\"\n",
    " \n",
    "# How the pipeline is used:\n",
    "logs = [\"1.1.1.1 500\", \"2.2.2.2 2500\", \"3.3.3.3 4000\"]\n",
    "records_gen = parse_logs(logs)\n",
    "heavy_ips = filter_heavy_hitters(records_gen) # Entire records_gen is consumed here\n",
    "report_lines = format_for_report(heavy_ips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a647cba6",
   "metadata": {},
   "source": [
    "def filter_heavy_hitters(records):\n",
    "    for ip, byte_count in records:\n",
    "        if byte_count > 1000:\n",
    "            yield ip\n",
    "\n",
    "\n",
    "Correct. By replacing the list-building logic with a for loop that yields values one by one, the function is transformed into a generator. This makes it lazy, processing one record at a time and passing it down the pipeline without building an intermediate list in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eac89fd",
   "metadata": {},
   "source": [
    "Question 6:\n",
    "An engineer is writing a helper function get_cloud_regions() that fetches a list of available cloud provider regions (for example, 'us-east-1', 'eu-west-2'). The list is small (fewer than 50 items), does not change during the program's execution, and needs to be referenced by multiple other functions throughout the application's lifecycle. Which approach is most suitable for this function and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a745b90",
   "metadata": {},
   "source": [
    "Answer: \n",
    "Use a regular function that returns a list because the data is small and needs to be accessed repeatedly. Storing the result in a list is more practical than re-creating an exhausted generator.\n",
    "\n",
    "\n",
    "Correct. For small, static datasets that need to be used more than once, the eager approach of returning a list (or tuple) is superior. The memory cost is trivial, and the resulting list can be reused, iterated over, and accessed by index freely, which is exactly what the scenario requires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9713e415",
   "metadata": {},
   "source": [
    "## Decorator Fundamentals and Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4405438c",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "In DevOps automation, you often need to add logging to many different functions that deploy services, update configurations, or run backups. Which statement best describes the primary advantage of using a decorator for this purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c928058",
   "metadata": {},
   "source": [
    "Answer: Decorators add functionality (a \"cross-cutting concern\" like logging or timing) to multiple functions without modifying each function's source code, thus avoiding code duplication.\n",
    "\n",
    "\n",
    "Correct. This is the core purpose of a decorator. It allows you to define a common behavior (like logging) in one place and apply it cleanly to any number of functions using the @ syntax, adhering to the \"Don't Repeat Yourself\" (DRY) principle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6571b8bd",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "What is the exact output of the following script when it is executed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87adc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    " \n",
    "def state_change_decorator(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper():\n",
    "        print(\"State: Preparing to execute...\")\n",
    "        func()\n",
    "        print(\"State: Execution complete.\")\n",
    "    return wrapper\n",
    " \n",
    "@state_change_decorator\n",
    "def apply_migrations():\n",
    "    print(\"Action: Applying database migrations.\")\n",
    " \n",
    "apply_migrations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61416c6f",
   "metadata": {},
   "source": [
    "State: Preparing to execute...\n",
    "Action: Applying database migrations.\n",
    "State: Execution complete.\n",
    "\n",
    "\n",
    "Correct. The @state_change_decorator syntax means apply_migrations now refers to the wrapper function. When called, wrapper first prints its \"before\" message, then calls the original apply_migrations function, and finally prints its \"after\" message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf0fb4e",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "An engineer wrote a decorator to add a prefix to a function's string result. However, when the script runs, the final line prints Result from main script: None. What is the cause of the bug, and what is the correct fix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a314bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        original_result = func(*args, **kwargs)\n",
    "        prefixed_result = f\"[PREFIX] {original_result}\"\n",
    "        print(f\"Inside wrapper, prefixed result is: {prefixed_result}\")\n",
    "    return wrapper\n",
    " \n",
    "@add_prefix_decorator\n",
    "def get_hostname(server_id):\n",
    "    return f\"server-{server_id}.prod.local\"\n",
    " \n",
    "hostname = get_hostname(101)\n",
    "print(f\"Result from main script: {hostname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d533adfc",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Cause: The wrapper function calculates the prefixed_result but does not return it. Since the wrapper has no explicit return, it implicitly returns None.\n",
    "Fix: Add return prefixed_result to the end of the wrapper function.\n",
    "\n",
    "\n",
    "\n",
    "Correct. The wrapper function effectively replaces the original function. If the wrapper doesn't return a value, the caller will receive None. The fix is to ensure the wrapper returns the intended new value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d142d9f9",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "This script uses a dictionary as a dispatch table to execute an action based on a command string. This pattern relies on functions being \"first-class citizens.\" What is the output of the script?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a688a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def provision_vm(hostname):\n",
    "    return f\"PROVISION: Virtual machine {hostname} created.\"\n",
    " \n",
    "def deprovision_vm(hostname):\n",
    "    return f\"DEPROVISION: Virtual machine {hostname} destroyed.\"\n",
    " \n",
    "def reboot_vm(hostname):\n",
    "    return f\"REBOOT: Virtual machine {hostname} restarting.\"\n",
    " \n",
    "action_map = {\n",
    "    \"create\": provision_vm,\n",
    "    \"destroy\": deprovision_vm,\n",
    "    \"restart\": reboot_vm,\n",
    "}\n",
    " \n",
    "command = \"create\"\n",
    "target = \"db-main-01\"\n",
    " \n",
    "if command in action_map:\n",
    "    # Look up the function and call it\n",
    "    action_func = action_map[command]\n",
    "    result = action_func(target)\n",
    "    print(result)\n",
    "else:\n",
    "    print(f\"Unknown command: {command}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef33fb98",
   "metadata": {},
   "source": [
    "PROVISION: Virtual machine db-main-01 created.\n",
    "Correct\n",
    "\n",
    "Correct. The script looks up the key \"create\" in action_map, which returns the provision_vm function object. It then calls this function with the argument \"db-main-01\", and the string returned by that function is printed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb07c26",
   "metadata": {},
   "source": [
    "Question 5:\n",
    "This decorator is designed to log details about a function call, including arguments and the return value. What will be printed to the console when this script is executed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79801df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUDIT: User 'dev-ops' is calling 'set_firewall_rule' with args (101,).\n",
      "AUDIT: Call finished. Returned: {'status': 'success', 'rule_id': 101, 'action': 'BLOCK'}\n"
     ]
    }
   ],
   "source": [
    "from functools import wraps\n",
    " \n",
    "def audit_log(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        user = kwargs.get(\"user\", \"unknown\")\n",
    "        print(f\"AUDIT: User '{user}' is calling '{func.__name__}' with args {args}.\")\n",
    "        value = func(*args, **kwargs)\n",
    "        print(f\"AUDIT: Call finished. Returned: {value}\")\n",
    "        return value\n",
    "    return wrapper\n",
    " \n",
    "@audit_log\n",
    "def set_firewall_rule(rule_id, action=\"BLOCK\", *, user=\"admin\"):\n",
    "    return {\"status\": \"success\", \"rule_id\": rule_id, \"action\": action}\n",
    " \n",
    "# Function is called here\n",
    "output = set_firewall_rule(101, user=\"dev-ops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00248be",
   "metadata": {},
   "source": [
    "Correct. The positional argument 101 is captured in args. The keyword argument user=\"dev-ops\" is captured in kwargs, and kwargs.get(\"user\") correctly retrieves it. The decorator prints its logs before and after calling the function, which uses its default value for action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f152763",
   "metadata": {},
   "source": [
    "## Advanced Decorator Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512382ec",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "What is the primary purpose and benefit of using @functools.wraps when creating a Python decorator?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2005e0f",
   "metadata": {},
   "source": [
    "\n",
    "It copies metadata (like __name__, __doc__, and the signature) from the original function to the wrapper function, which is crucial for introspection, documentation, and debugging.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. Without @wraps, calling help() or accessing __name__ on a decorated function would show information about the inner wrapper function, not the original function. @wraps fixes this by making the wrapper \"look\" like the original function from the outside."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e61f3",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "An engineer writes a decorator to log function calls, but forgets to use @functools.wraps. What will be the output when the following script inspects the decorated function's metadata?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a39e4abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function name: wrapper_function\n",
      "Docstring: This is the wrapper's docstring.\n"
     ]
    }
   ],
   "source": [
    "def logging_decorator(original_function):\n",
    "    def wrapper_function(*args, **kwargs):\n",
    "        \"\"\"This is the wrapper's docstring.\"\"\"\n",
    "        print(f\"Calling function: {original_function.__name__}\")\n",
    "        return original_function(*args, **kwargs)\n",
    "    return wrapper_function\n",
    " \n",
    "@logging_decorator\n",
    "def get_user_permissions(user_id: int) -> list:\n",
    "    \"\"\"Returns a list of permissions for a given user.\"\"\"\n",
    "    return [\"admin\", \"editor\"]\n",
    " \n",
    "print(f\"Function name: {get_user_permissions.__name__}\")\n",
    "print(f\"Docstring: {get_user_permissions.__doc__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d28f46",
   "metadata": {},
   "source": [
    "Correct. Because @functools.wraps was not used, the decorated get_user_permissions variable now points directly to wrapper_function. Therefore, inspecting its __name__ and __doc__ attributes reveals the metadata of the wrapper, not the original function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3c625c",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "When designing a decorator, you realize you need to pass a configuration value to it at definition time, like so: @retry(attempts=5). Why is an extra layer of nesting (a \"decorator factory\") required to achieve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec3e3b",
   "metadata": {},
   "source": [
    "\n",
    "Because a standard decorator function is only allowed to accept the function it decorates. To accept other arguments, you must call a factory function that returns the actual decorator.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. A plain decorator's signature is decorator(func). The @ syntax provides only that one argument (the function being decorated). The expression @retry(attempts=5) first calls retry(attempts=5), which must return the real decorator. That returned decorator then receives the function to be decorated. This three-layer structure (factory -> decorator -> wrapper) is necessary to handle configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477daeaf",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "This script uses a configurable decorator require_role to protect a function. Given the decorator and the function call, what is the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e02fc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permission denied. Requires 'admin'.\n",
      "Final result: None\n"
     ]
    }
   ],
   "source": [
    "from functools import wraps\n",
    " \n",
    "CURRENT_USER = {\"username\": \"prod-agent\", \"role\": \"viewer\"}\n",
    " \n",
    "def require_role(required_role):\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            if CURRENT_USER.get(\"role\") == required_role:\n",
    "                print(\"Permission granted.\")\n",
    "                return func(*args, **kwargs)\n",
    "            else:\n",
    "                print(f\"Permission denied. Requires '{required_role}'.\")\n",
    "                return None\n",
    "        return wrapper\n",
    "    return decorator\n",
    " \n",
    "@require_role(required_role=\"admin\")\n",
    "def deploy_service(service_name):\n",
    "    print(f\"Deploying service: {service_name}\")\n",
    "    return \"SUCCESS\"\n",
    " \n",
    "result = deploy_service(\"user-database\")\n",
    "print(f\"Final result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f00caa",
   "metadata": {},
   "source": [
    "Correct. The @require_role(required_role=\"admin\") call configures the decorator to check for the \"admin\" role. The wrapper compares this to the CURRENT_USER's role (\"viewer\") and finds a mismatch. It then prints the denial message and returns None, which is then printed as the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1f060c",
   "metadata": {},
   "source": [
    "Question 5:\n",
    "An engineer attempts to write a decorator factory with_context that adds a context dictionary to the decorated function's keyword arguments. However, the implementation is structurally incorrect. Which of the following implementations correctly fixes the structure of the with_context decorator factory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72676708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_context(func, context_data):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        kwargs['context'] = context_data\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper\n",
    " \n",
    " \n",
    "@with_context(context_data={\"user_id\": 123})\n",
    "def process_request(request_id, *, context={}):\n",
    "    print(f\"Processing {request_id} for user {context['user_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97a5dbb",
   "metadata": {},
   "source": [
    "def with_context(context_data):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            kwargs['context'] = context_data\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "Correct\n",
    "\n",
    "Correct. This code correctly implements the three-level structure. with_context(context_data) is the factory that accepts configuration. It returns decorator(func), which is the actual decorator that accepts the function. decorator in turn returns wrapper, which contains the runtime logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bba5ff",
   "metadata": {},
   "source": [
    "Question 6:\n",
    "When stacking multiple decorators on a single function, the order in which they are listed matters. In which scenario is the order most critical and likely to cause bugs or unexpected behavior?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6223f5b9",
   "metadata": {},
   "source": [
    "When one decorator transforms the return value (e.g., formats it as JSON) and another decorator needs to operate on that transformed value (e.g., logs the JSON string).\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the most critical case. If a decorator expects a certain data type (e.g., a dictionary) but the decorator below it in the stack has already converted it to another type (e.g., a JSON string), a TypeError or AttributeError will occur. The execution order (top-down) means the outer decorator acts on the result of the inner decorator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c52e9a",
   "metadata": {},
   "source": [
    "Question 7:\n",
    "Two decorators, @log_enter and @log_exit, are stacked on a function. Based on the rules of decorator application and execution, what is the exact output printed to the console?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e5801b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTERING\n",
      "  ...TASK RUNNING...\n",
      "EXITING\n"
     ]
    }
   ],
   "source": [
    "from functools import wraps\n",
    " \n",
    "def log_enter(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(\"ENTERING\")\n",
    "        return func(*args, **kwargs)\n",
    " \n",
    "    return wrapper\n",
    " \n",
    "def log_exit(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        result = func(*args, **kwargs)\n",
    "        print(\"EXITING\")\n",
    "        return result\n",
    " \n",
    "    return wrapper\n",
    " \n",
    "@log_enter\n",
    "@log_exit\n",
    "def perform_task():\n",
    "    print(\"  ...TASK RUNNING...\")\n",
    " \n",
    "perform_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be6300f",
   "metadata": {},
   "source": [
    "Correct. Execution happens from the outside-in. The @log_enter decorator is outermost, so its wrapper runs first, printing \"ENTERING\". It then calls the next function in the chain (the one wrapped by @log_exit). The @log_exit wrapper calls the original perform_task, which prints \"...TASK RUNNING...\". Finally, the execution unwinds: the @log_exit wrapper prints \"EXITING\", and the call chain completes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92a99c2",
   "metadata": {},
   "source": [
    "## Python Exception Handling: The Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d6b4cc",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "A DevOps engineer writes a Python function to check the status of a service endpoint. What is the exact output returned by the function call check_service_status(80)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9a730e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating check...\n",
      "Check completed without errors.\n",
      "Finalizing check.\n",
      "['OK', 'SUCCESS', 'LOGGED']\n"
     ]
    }
   ],
   "source": [
    "def check_service_status(port):\n",
    "    print(\"Initiating check...\")\n",
    "    log = []\n",
    "    try:\n",
    "        if port == 80:\n",
    "            log.append(\"OK\")\n",
    "        elif port == 503:\n",
    "            raise ConnectionRefusedError(\"Service unavailable\")\n",
    "        else:\n",
    "            log.append(\"Unknown\")\n",
    "    except ConnectionRefusedError:\n",
    "        log.append(\"FAIL\")\n",
    "        print(\"Connection failed.\")\n",
    "    else:\n",
    "        log.append(\"SUCCESS\")\n",
    "        print(\"Check completed without errors.\")\n",
    "    finally:\n",
    "        log.append(\"LOGGED\")\n",
    "        print(\"Finalizing check.\")\n",
    "    return log\n",
    "\n",
    "print(check_service_status(80))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3c3e3a",
   "metadata": {},
   "source": [
    "Correct. When port is 80, the if port == 80: condition is met, and \"OK\" is appended. No exception is raised, so the except block is skipped. Because the try block completed without raising an exception, the else block is executed, appending \"SUCCESS\" and printing a message. The finally block always executes, appending \"LOGGED\" and printing its message. The final list returned is ['OK', 'SUCCESS', 'LOGGED']."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b88fcf1",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "A script is being designed to process a list of hostnames. For each hostname, it will connect, download a configuration file, and parse it. The network is known to be unreliable, and some configuration files might be malformed or missing expected keys.\n",
    "\n",
    "Which of the following best describes the most Pythonic and robust design philosophy for this script?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd85013",
   "metadata": {},
   "source": [
    "The EAFP (Easier to Ask for Forgiveness than Permission) approach, where the script directly attempts to connect, download, and parse the file inside a try block, using specific except blocks to handle ConnectionError, FileNotFoundError, or KeyError as they occur.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the preferred Pythonic approach for this scenario. It results in cleaner, more readable code by focusing the try block on the primary task (the \"happy path\"). It robustly handles specific, expected errors in separate except blocks, clearly separating the main logic from the error-handling logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed3e141",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "You are reviewing a script that parses structured log data, which is represented as a list of dictionaries. What will be printed to the console when this script is executed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4dad770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['101:alice', '102:system', 'ERROR:Invalid-Entry', 'ERROR:Missing-Data']\n"
     ]
    }
   ],
   "source": [
    "def summarize_logs(log_entries):\n",
    "    summary = []\n",
    "    for entry in log_entries:\n",
    "        try:\n",
    "            # The 'user' key is optional\n",
    "            user = entry.get('user', 'system')\n",
    "            # The 'event_id' key is mandatory\n",
    "            event_id = entry['event_id']\n",
    "            summary.append(f\"{event_id}:{user}\")\n",
    "        except KeyError:\n",
    "            summary.append(\"ERROR:Missing-Data\")\n",
    "        except (TypeError, AttributeError):\n",
    "            summary.append(\"ERROR:Invalid-Entry\")\n",
    " \n",
    "    return summary\n",
    " \n",
    "logs = [\n",
    "    {'event_id': 101, 'user': 'alice'},\n",
    "    {'event_id': 102},\n",
    "    None,\n",
    "    {'user': 'bob'}\n",
    "]\n",
    " \n",
    "print(summarize_logs(logs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20aa881",
   "metadata": {},
   "source": [
    "Correct. The third entry None causes a TypeError when the code attempts entry.get(...), as NoneType has no .get method. The except TypeError block is triggered. Result: 'ERROR:Invalid-Entry'. The fourth entry {'user': 'bob'} causes a KeyError when the code attempts entry['event_id'], as this key is missing. The except KeyError block is triggered. Result: 'ERROR:Missing-Data'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c588b610",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "A junior developer wrote the following function to retrieve a specific metric from a nested dictionary representing server monitoring data. The function call returns \"Could not retrieve metric 'memory' for server 'srv-db-01'.\" as expected. However, what is the most significant flaw in this function's error handling design?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d8f139f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve metric 'memory' for server 'srv-db-01'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_metric(data, server_id, metric_name):\n",
    "    try:\n",
    "        value = data[server_id][metric_name]\n",
    "        return f\"Metric '{metric_name}' on server '{server_id}' is {value}\"\n",
    "    except:\n",
    "        return f\"Could not retrieve metric '{metric_name}' for server '{server_id}'.\"\n",
    " \n",
    "# Example usage:\n",
    "server_data = {\n",
    "    \"srv-web-01\": {\"cpu\": 0.75, \"memory\": 0.5},\n",
    "    \"srv-db-01\": {\"cpu\": 0.4}\n",
    "}\n",
    "print(get_metric(server_data, \"srv-db-01\", \"memory\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d09954",
   "metadata": {},
   "source": [
    "\n",
    "Using a bare except: clause is a bad practice because it catches all exceptions, including system-level ones and programming errors, making the code difficult to debug.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the most critical flaw. A bare except: (equivalent to except BaseException:) catches everything, including SystemExit, KeyboardInterrupt, and even SyntaxError. More importantly for a developer, it will silently hide unrelated bugs, such as a TypeError if data was accidentally passed as None. The handler should have been specific, like except KeyError:, to only catch the expected error of a missing key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452b1c67",
   "metadata": {},
   "source": [
    "Question 5:\n",
    "What is the primary conceptual difference between a ValueError and a TypeError in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0860a070",
   "metadata": {},
   "source": [
    "\n",
    "ValueError occurs when an operation receives an argument of the right type but an inappropriate value, whereas TypeError occurs when the argument's type itself is invalid for the operation.\n",
    "\n",
    "Correct\n",
    "\n",
    "This is the core distinction. For int(\"abc\"), the int() function is given a str (the correct type), but the value \"abc\" is inappropriate. This is a ValueError. For len(123), the len() function is given an int, which is the wrong type entirely; it expects a sequence or collection. This is a TypeError."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386a49c8",
   "metadata": {},
   "source": [
    "## Raising and Defining custom exceptions in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b5433",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "You are writing a function get_user_profile(user_id) that fetches user data from a remote database. Under which circumstances is it most appropriate to raise an exception rather than return None?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c39e31",
   "metadata": {},
   "source": [
    "\n",
    "When the database connection times out or is refused, the function should raise ConnectionError.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. A connection failure is an unexpected, exceptional event that prevents the function from fulfilling its purpose. The function itself cannot resolve this issue. Raising an exception is the correct way to signal this failure to the caller, so it can be logged, retried, or handled appropriately at a higher level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b805774",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "A function is written to validate a deployment configuration. Analyze its behavior with the given inputs. What is the output of this script?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f32b2982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config valid: 1024MB\n",
      "Error: Memory limit 128 is outside the allowed range (256-4096).\n",
      "Error: Mandatory key 'memory_limit_mb' is missing.\n",
      "Error: Configuration must be a dictionary.\n"
     ]
    }
   ],
   "source": [
    "def validate_config(config_data):\n",
    "    if not isinstance(config_data, dict):\n",
    "        raise TypeError(\"Configuration must be a dictionary.\")\n",
    " \n",
    "    mem_limit = config_data.get(\"memory_limit_mb\")\n",
    "    if mem_limit is None:\n",
    "        raise ValueError(\"Mandatory key 'memory_limit_mb' is missing.\")\n",
    " \n",
    "    if not (isinstance(mem_limit, int) and 256 <= mem_limit <= 4096):\n",
    "        raise ValueError(f\"Memory limit {mem_limit} is outside the allowed range (256-4096).\")\n",
    " \n",
    "    print(f\"Config valid: {mem_limit}MB\")\n",
    " \n",
    " \n",
    "configs = [\n",
    "    {\"memory_limit_mb\": 1024},\n",
    "    {\"memory_limit_mb\": 128},\n",
    "    {\"cpu_cores\": 4},\n",
    "    \"memory_limit_mb: 512\"\n",
    "]\n",
    " \n",
    "for config in configs:\n",
    "    try:\n",
    "        validate_config(config)\n",
    "    except (ValueError, TypeError) as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17b2e6a",
   "metadata": {},
   "source": [
    "{\"memory_limit_mb\": 1024}: Passes all checks. Prints \"Config valid...\". {\"memory_limit_mb\": 128}: Fails the range check 256 <= 128. A ValueError is raised and caught. {\"cpu_cores\": 4}: mem_limit is None. A ValueError is raised due to the missing key and caught. \"memory_limit_mb: 512\": This is a string, not a dictionary. The first isinstance check fails, raising a TypeError which is caught."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afe660c",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "Which of the following is the most significant advantage of defining a custom exception hierarchy, such as a base ServiceError with subclasses AuthenticationError and QuotaExceededError?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8aaf80",
   "metadata": {},
   "source": [
    "It allows the calling code to use a single except ServiceError: block to handle all service-related failures, or to use specific except AuthenticationError: blocks for targeted recovery logic.\n",
    "\n",
    "Correct\n",
    "\n",
    "This is the primary benefit. A custom hierarchy provides flexibility. Callers can handle errors at different levels of granularity: catch a specific subclass to retry or refresh a token, or catch the base class to perform generic logging and cleanup for any related failure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f5fae9",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "A script uses a custom exception hierarchy to manage errors during a file provisioning process. What is the output of this script?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac66f7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught Provisioner Error: Cannot write to protected path: /root/data.bin\n"
     ]
    }
   ],
   "source": [
    "class ProvisionerError(Exception):\n",
    "    \"\"\"Base class for provisioning failures.\"\"\"\n",
    "    pass\n",
    " \n",
    "class DiskSpaceError(ProvisionerError):\n",
    "    \"\"\"Raised when there is not enough disk space.\"\"\"\n",
    "    def __init__(self, required, available):\n",
    "        super().__init__(f\"Not enough disk. Required: {required}GB, Available: {available}GB\")\n",
    " \n",
    "class PermissionsError(ProvisionerError):\n",
    "    \"\"\"Raised due to file system permission issues.\"\"\"\n",
    "    pass\n",
    " \n",
    "def provision_file(size_gb, path):\n",
    "    if size_gb > 100:\n",
    "        raise DiskSpaceError(required=size_gb, available=100)\n",
    "    if \"/root/\" in path:\n",
    "        raise PermissionsError(f\"Cannot write to protected path: {path}\")\n",
    "    print(\"Provisioning successful.\")\n",
    " \n",
    "try:\n",
    "    provision_file(size_gb=50, path=\"/root/data.bin\")\n",
    "except DiskSpaceError as e:\n",
    "    print(f\"Caught Disk Error: {e}\")\n",
    "except ProvisionerError as e:\n",
    "    print(f\"Caught Provisioner Error: {e}\")\n",
    "except Exception:\n",
    "    print(\"Caught a generic exception.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8cc209",
   "metadata": {},
   "source": [
    "Correct. The function raises a PermissionsError. The first except block (except DiskSpaceError) does not match. The second except block (except ProvisionerError) does match, because PermissionsError is a subclass of ProvisionerError. Execution enters this block and prints the message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f9a752",
   "metadata": {},
   "source": [
    "Question 5:\n",
    "You are developing a parser for a custom configuration file format. You've created a special exception to handle syntax errors. What will be printed to the console after the following code executes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82050442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntax error on line 2: Missing '=' assignment\n",
      "-> Problematic text: 'port'\n"
     ]
    }
   ],
   "source": [
    "class ConfigSyntaxError(ValueError):\n",
    "    def __init__(self, message, line_num, text):\n",
    "        full_msg = f\"Syntax error on line {line_num}: {message}\"\n",
    "        super().__init__(full_msg)\n",
    "        self.line = line_num\n",
    "        self.text = text\n",
    " \n",
    "def parse_config(lines):\n",
    "    for i, line in enumerate(lines, 1):\n",
    "        if \"=\" not in line:\n",
    "            raise ConfigSyntaxError(\"Missing '=' assignment\", i, line)\n",
    "    return \"Parsed OK\"\n",
    " \n",
    "config_text = [\"host=server.local\", \"port\", \"timeout=30\"]\n",
    " \n",
    "try:\n",
    "    parse_config(config_text)\n",
    "except ConfigSyntaxError as e:\n",
    "    print(e)\n",
    "    print(f\"-> Problematic text: '{e.text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972daf32",
   "metadata": {},
   "source": [
    "Correct. The parse_config function finds that the second line (\"port\") does not contain an =. It raises a ConfigSyntaxError, passing the message, line number (2), and the text. The except block catches it. The first print(e) prints the message generated by super().__init__(). The second print statement accesses the custom .text attribute on the exception object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d07b8df",
   "metadata": {},
   "source": [
    "Question 6:\n",
    "Analyze the following function signature and its internal logic.\n",
    "\n",
    "\n",
    "\n",
    "def process_data_files(file_paths):\n",
    "    # (function implementation)\n",
    "\n",
    "\n",
    "Which of the following implementations best demonstrates the principle of raising an exception for a true error state versus handling a valid-but-empty edge case gracefully?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc44b7",
   "metadata": {},
   "source": [
    "if not isinstance(file_paths, list):\n",
    "    raise TypeError(\"Input must be a list of file paths.\")\n",
    "if not file_paths:\n",
    "    print(\"No files to process.\")\n",
    "    return\n",
    "# ... process files ...\n",
    "Correct\n",
    "\n",
    "This implementation correctly identifies two different situations. An input that is not a list is a contract violation (a true error), which is correctly handled by raising a TypeError. An empty list is a valid edge case that is handled gracefully by printing a message and exiting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832b85f6",
   "metadata": {},
   "source": [
    "## Resource Management with Context Managers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df7a66c",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "A DevOps script needs to open a log file, write several status updates to it, and ensure the file is always closed, even if a network error interrupts the script mid-operation.\n",
    "\n",
    "Which of the following two code blocks represents the best practice for this task in Python, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f8e0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Block A:\n",
    "\n",
    "f = open(\"update.log\", \"w\")\n",
    "try:\n",
    "    f.write(\"Starting...\\n\")\n",
    "    # ... network operations that might fail ...\n",
    "    f.write(\"Finished.\\n\")\n",
    "finally:\n",
    "    f.close()\n",
    "\n",
    "\n",
    "Block B:\n",
    "\n",
    "with open(\"update.log\", \"w\") as f:\n",
    "    f.write(\"Starting...\\n\")\n",
    "    # ... network operations that might fail ...\n",
    "    f.write(\"Finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b201b52",
   "metadata": {},
   "source": [
    "\n",
    "Block B is better because the with statement is more concise, less error-prone, and guarantees the resource (f) is cleaned up (closed) upon exiting the block for any reason.\n",
    "\n",
    "Correct\n",
    "\n",
    "The with statement is the Pythonic best practice for resource management. It encapsulates the try...finally logic, making the code cleaner, more readable, and safer by guaranteeing that the resource's teardown logic (in this case, f.close()) is automatically executed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a4e3d2",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "A developer writes a custom context manager to temporarily switch the current working directory for a script. However, they notice that if an error occurs during the file operations, the script does not revert to the original directory. What is the primary bug in the temp_directory context manager?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5438a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "import os\n",
    " \n",
    "@contextmanager\n",
    "def temp_directory(path):\n",
    "    original_dir = os.getcwd()\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    os.chdir(path)\n",
    "    yield\n",
    "    os.chdir(original_dir)\n",
    "    os.rmdir(path)\n",
    "    print(f\"Reverted to {original_dir}\")\n",
    " \n",
    "try:\n",
    "    with temp_directory(\"./local_temp_folder\"):\n",
    "        print(f\"Now in: {os.getcwd()}\")\n",
    "        result = 1 / 0  # Simulate an error\n",
    "except ZeroDivisionError:\n",
    "    print(\"An error occurred.\")\n",
    " \n",
    "print(f\"Final directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43265cb2",
   "metadata": {},
   "source": [
    "\n",
    "The teardown logic (os.chdir(original_dir)) is not placed inside a finally block, so it is skipped when an exception is raised in the with block.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the critical bug. When the ZeroDivisionError occurs inside the with block, the generator's execution is terminated. Code that comes after the yield will only be executed if it is in a finally clause. The fix is to wrap the yield in a try...finally block to guarantee the cleanup code runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a44915f",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "Analyze the following custom context manager class designed to handle database transactions. It is designed to suppress IntegrityError (e.g., duplicate key) while letting other errors propagate. What is the exact output of this script?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7af0b3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN TRANSACTION\n",
      "ROLLBACK: An unexpected error occurred.\n",
      "Handled at top level: Connection lost\n"
     ]
    }
   ],
   "source": [
    "class DatabaseTransaction:\n",
    "    def __enter__(self):\n",
    "        print(\"BEGIN TRANSACTION\")\n",
    "        return self\n",
    " \n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        if exc_type is None:\n",
    "            print(\"COMMIT\")\n",
    "            return False  # Propagate no exception\n",
    "        elif exc_type is IntegrityError:\n",
    "            print(\"ROLLBACK: Ignoring duplicate key.\")\n",
    "            return True  # Suppress this specific exception\n",
    "        else:\n",
    "            print(\"ROLLBACK: An unexpected error occurred.\")\n",
    "            return False  # Propagate other exceptions\n",
    " \n",
    " \n",
    "class IntegrityError(Exception):\n",
    "    pass\n",
    " \n",
    " \n",
    "class NetworkError(Exception):\n",
    "    pass\n",
    " \n",
    "try:\n",
    "    with DatabaseTransaction():\n",
    "        raise NetworkError(\"Connection lost\")\n",
    "except NetworkError as e:\n",
    "    print(f\"Handled at top level: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ea39d1",
   "metadata": {},
   "source": [
    "Correct. __enter__ is called, printing \"BEGIN TRANSACTION\". A NetworkError is raised inside the with block. __exit__ is called. exc_type is NetworkError. It doesn't match IntegrityError, so it falls to the else clause, printing \"ROLLBACK: An unexpected error occurred.\". __exit__ returns False, so the NetworkError is re-raised and propagated outside the with statement. The outer except NetworkError block catches it and prints the final message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412e583e",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "When designing a custom context manager, what is the key difference in purpose between implementing it as a class with __enter__/__exit__ methods versus using the @contextlib.contextmanager decorator on a generator?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf27f95",
   "metadata": {},
   "source": [
    "The class-based approach is generally better for managing complex state or when the setup/teardown logic is substantial, while the decorator is more suitable for simpler, more linear setup/teardown tasks.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This accurately describes the trade-off. A class provides a natural way to manage complex state through its attributes (self). For simple cases like temporarily changing a directory or setting an environment variable, the decorator is more concise and requires less boilerplate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c687b1a8",
   "metadata": {},
   "source": [
    "Question 5:\n",
    "Considering the following code, what is the purpose of the value returned by the __enter__ method in a class-based context manager?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4ce70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApiSessionManager:\n",
    "    def __init__(self, api_key):\n",
    "        self.key = api_key\n",
    "        self.session_id = None\n",
    " \n",
    "    def __enter__(self):\n",
    "        print(\"Starting API session...\")\n",
    "        self.session_id = \"xyz-123\"\n",
    "        return self # Returns the instance itself\n",
    " \n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        print(\"Closing API session...\")\n",
    "        self.session_id = None\n",
    " \n",
    "with ApiSessionManager(\"my-secret-key\") as session:\n",
    "    print(f\"Using session ID: {session.session_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3dfde6",
   "metadata": {},
   "source": [
    "It is the value that gets assigned to the variable specified in the as clause of the with statement.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. The with ... as var: syntax captures the return value of the __enter__ method in the variable var. In this example, session becomes a reference to the ApiSessionManager instance, allowing access to its attributes like session_id."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0839f5",
   "metadata": {},
   "source": [
    "## Python Logging: Core Concepts and Mechanics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e59acd7",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "A DevOps team is developing a long-running Python service to manage cloud infrastructure. For debugging and auditing, they need to record events such as service startup, configuration reloads, and connection failures. Why is using Python's logging module a better practice than using print() statements for this purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c3090a",
   "metadata": {},
   "source": [
    "\n",
    "logging allows developers to categorize messages by severity (e.g., DEBUG, INFO, WARNING, ERROR), which can be filtered and routed to different destinations, whereas print() treats all messages with the same importance.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the core advantage. Log levels allow for granular control over log verbosity. In production, a team might only want to see INFO and above, but in a staging environment, they can enable DEBUG logs for deep diagnostics, all without changing the application code. This is impossible to achieve cleanly with print()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdac07db",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "Which log level is most appropriate for recording a user's password during a failed login attempt for debugging purposes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f998b18",
   "metadata": {},
   "source": [
    "Passwords and other sensitive credentials should never be logged in plain text, regardless of the log level.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the fundamental security best practice. Logging sensitive information like passwords, API keys, or personal data creates a major security risk. If you need to confirm a value was received, you can log a confirmation like password_provided: True or a salted hash, but never the raw value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8291e87a",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "You are configuring a logger for a script that monitors system metrics. You want to see only high-severity alerts on the console. What will be printed to the console when this script is executed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602d33c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    " \n",
    "monitor_logger = logging.getLogger(\"system.monitor\")\n",
    "monitor_logger.setLevel(logging.INFO)\n",
    " \n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setLevel(logging.ERROR)\n",
    "console_handler.setFormatter(logging.Formatter(logging.BASIC_FORMAT))\n",
    " \n",
    "monitor_logger.addHandler(console_handler)\n",
    " \n",
    "monitor_logger.info(\"CPU usage is at 25%.\")\n",
    "monitor_logger.warning(\"Memory usage is at 85%.\")\n",
    "monitor_logger.error(\"Disk space is critically low.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c535233",
   "metadata": {},
   "source": [
    "ERROR:system.monitor:Disk space is critically low.\n",
    "Correct\n",
    "\n",
    "Correct. This demonstrates two-stage filtering. The logger's level is INFO, so it accepts INFO, WARNING, and ERROR messages. These messages are passed to the handler. The handler's level is ERROR. It will discard any message with a severity lower than ERROR. Therefore, the INFO and WARNING messages are discarded by the handler, and only the ERROR message is processed and printed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a994ed9",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "A junior developer writes a script to log important events. After running it, they see that the audit.log file is created, but it is empty. The expected message \"User 'admin' logged in.\" does not appear in the file. What is the fundamental reason the log message is not written to the file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210cfb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    " \n",
    "def setup_auditing():\n",
    "    audit_logger = logging.getLogger(\"audit_trail\")\n",
    "    audit_logger.setLevel(logging.INFO)\n",
    " \n",
    "    audit_handler = logging.FileHandler(\"audit.log\")\n",
    "    audit_handler.setLevel(logging.INFO)\n",
    " \n",
    "setup_auditing()\n",
    " \n",
    "logger = logging.getLogger(\"audit_trail\")\n",
    "logger.info(\"User 'admin' logged in.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec256e77",
   "metadata": {},
   "source": [
    "A logger without any handlers attached to it will not pass log records to any destination. The audit_handler was never attached using audit_logger.addHandler().\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the core issue. A logger's job is to pass log records to its list of handlers. In the code, audit_handler is created but never registered with the audit_logger. Because the logger has no handlers, it effectively discards the log message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27c65e7",
   "metadata": {},
   "source": [
    "Question 5:\n",
    "In the architecture of Python's logging module, what are the primary, distinct roles of the Logger, Handler, and Formatter components?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec0a975",
   "metadata": {},
   "source": [
    "Logger: Provides the main entry point for code to emit messages. Handler: Directs log records to a specific destination. Formatter: Defines the string layout of the final log record.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This correctly describes the separation of concerns. Logger: The interface the application code uses (logging.getLogger(...)). Handler: Responsible for the output destination (e.g., StreamHandler for console, FileHandler for files). Formatter: Responsible for the appearance of the log message (e.g., adding a timestamp, level name)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265d11a8",
   "metadata": {},
   "source": [
    "## Practical Logging: File Handlers and Structured JSON Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2833933d",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "A service running on a server generates approximately 1 GB of log data per day. A junior engineer suggests logging everything to a single service.log file. What is the primary reason for using a RotatingFileHandler or TimedRotatingFileHandler instead of a single, ever-growing file?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3b9e8d",
   "metadata": {},
   "source": [
    "\n",
    "Rotating logs into smaller, manageable chunks (e.g., daily or by size) prevents single files from consuming excessive disk space and makes it much easier to archive, search, or delete old logs.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the core benefit. A single 30 GB log file is difficult to open, search, or transfer. A collection of 30 daily 1 GB files is far more manageable. It simplifies log lifecycle management (e.g., \"delete logs older than 30 days\") and prevents a single file from filling a disk partition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e523cd25",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "A script is configured to log status messages with size-based rotation. The maxBytes is set very low to demonstrate the rotation behavior. After the script completes, what files will exist and what will be the content of app.log.2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a6411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import logging.handlers\n",
    "import os\n",
    " \n",
    "# --- Cleanup for predictable test runs ---\n",
    "for f in os.listdir('.'):\n",
    "    if f.startswith('app.log'):\n",
    "        os.remove(f)\n",
    " \n",
    "# --- Logger Setup ---\n",
    "logger = logging.getLogger('rotation_test')\n",
    "logger.setLevel(logging.INFO)\n",
    " \n",
    "formatter = logging.Formatter('%(message)s')\n",
    " \n",
    "handler = logging.handlers.RotatingFileHandler(\n",
    "    'app.log', maxBytes=50, backupCount=2)\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    " \n",
    "# --- Logging Calls ---\n",
    "for i in range(7):\n",
    "    logger.info(f\"Log entry number {i+1}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6caea7c",
   "metadata": {},
   "source": [
    "Files app.log, app.log.1, app.log.2. The content of app.log.2 will be Log entry number 3... followed by Log entry number 4....\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. Entries 1 and 2 (40 bytes) go into app.log. Entry 3 (20 bytes) triggers rotation. app.log is renamed to app.log.1. A new app.log is created and gets entry 3. Entry 4 (20 bytes) goes into the new app.log. Entry 5 (20 bytes) triggers rotation. app.log.1 is renamed to app.log.2. The current app.log (containing 3 & 4) is renamed to app.log.1. A new app.log gets entry 5. Entry 6 (20 bytes) goes into the new app.log. Entry 7 (20 bytes) triggers rotation. app.log.2 (containing 1 & 2) is deleted because backupCount is 2. app.log.1 (containing 3 & 4) is renamed to app.log.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b392c9e4",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "A developer is trying to set up a logger that rotates its log file every hour. However, the script crashes at startup with an AttributeError. What is the cause of the AttributeError and how should it be fixed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b050d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    " \n",
    "logger = logging.getLogger('hourly_reporter')\n",
    "logger.setLevel(logging.INFO)\n",
    " \n",
    "handler = logging.TimedRotatingFileHandler('reporter.log', when='h', interval=1)\n",
    " \n",
    "logger.addHandler(handler)\n",
    "logger.info(\"Reporter starting up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f39254",
   "metadata": {},
   "source": [
    "\n",
    "The TimedRotatingFileHandler class is not in the top-level logging module. It must be imported from the logging.handlers submodule.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the precise bug. Specialized handlers like TimedRotatingFileHandler and RotatingFileHandler reside in a submodule. The code should be import logging.handlers and handler = logging.handlers.TimedRotatingFileHandler(...). The AttributeError occurs because Python cannot find TimedRotatingFileHandler as an attribute of the main logging module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb575d4",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "Your team is building a microservices-based application where logs from dozens of services are sent to a central log aggregation platform like Splunk or Elasticsearch. Why is structured logging (e.g., in JSON format) strongly preferred over plain-text logging in this environment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4698d760",
   "metadata": {},
   "source": [
    "\n",
    "Structured logs are human-readable key-value pairs that can be reliably parsed by machines, allowing for powerful filtering, querying, and aggregation. Plain-text logs require brittle and slow regular expressions to parse.\n",
    "\n",
    "Correct\n",
    "\n",
    "This is the primary advantage. With structured logs, fields like level, user_id, or request_id are distinct data points. This allows the aggregation platform to index them for fast, reliable queries. Parsing plain text is inefficient and breaks every time a developer changes the log message format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f28b1ec",
   "metadata": {},
   "source": [
    "Question 5:\n",
    "You are setting up a JSON logger to include a custom request ID and to rename some default fields for clarity. What is the exact JSON output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "457618cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"severity\": \"INFO\", \"logger_name\": \"api_gateway\", \"message\": \"Incoming request processed\", \"request_id\": \"abc-xyz-789\", \"method\": \"GET\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import logging\n",
    "import sys\n",
    "from pythonjsonlogger.json import JsonFormatter\n",
    " \n",
    "logger = logging.getLogger('api_gateway')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers.clear()\n",
    " \n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "formatter = JsonFormatter(\n",
    "    '%(levelname) %(name) %(message)s',\n",
    "    rename_fields={'levelname': 'severity', 'name': 'logger_name'}\n",
    ")\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    " \n",
    "logger.info(\n",
    "    'Incoming request processed',\n",
    "    extra={'request_id': 'abc-xyz-789', 'method': 'GET'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719117f3",
   "metadata": {},
   "source": [
    "{\"severity\": \"INFO\", \"logger_name\": \"api_gateway\", \"message\": \"Incoming request processed\", \"request_id\": \"abc-xyz-789\", \"method\": \"GET\"}\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. The JsonFormatter creates a JSON object where: levelname is renamed to severity and has the value \"INFO\". name is renamed to logger_name and has the value \"api_gateway\". message has the value \"Incoming request processed\". All key-value pairs from the extra dictionary are added as top-level fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13261e1",
   "metadata": {},
   "source": [
    "Question 6:\n",
    "A developer is trying to add a task_id and worker_id to their JSON logs for better traceability. However, these fields are not appearing in the final log output. Why are the task_id and worker_id fields missing from the JSON output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae1fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from pythonjsonlogger.json import JsonFormatter\n",
    " \n",
    "logger = logging.getLogger(\"worker_pool\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers.clear()\n",
    " \n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "formatter = JsonFormatter()\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    " \n",
    "context_data = {\"task_id\": \"t-456\", \"worker_id\": \"w-03\"}\n",
    " \n",
    "logger.info(\"Task completed successfully\", context_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b37cf22",
   "metadata": {},
   "source": [
    "\n",
    "The context dictionary must be passed as the extra keyword argument, not as a positional argument. The correct call is logger.info('...', extra=context_data).\n",
    "\n",
    "Correct\n",
    "\n",
    "The logging methods (info, warning, etc.) interpret the second positional argument as data for string formatting within the message itself (e.g., logger.info(\"User %s logged in\", username)). To pass a dictionary of context to be added to the log record, it must be passed using the extra keyword argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827dab3",
   "metadata": {},
   "source": [
    "## Declarative and Dynamic Logging Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d900a1",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "What is the primary architectural advantage of using a declarative configuration method (like logging.config.dictConfig or fileConfig) over an imperative one (programmatically creating and attaching handlers/formatters)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3290803e",
   "metadata": {},
   "source": [
    "\n",
    "It separates the logging configuration (the \"what\") from the application's business logic (the \"how\"), allowing the logging setup to be modified without changing the application code.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the core principle. By externalizing the configuration into a file (like a .ini or .json), an operations team can adjust log levels, change output destinations, or modify formats for a running application just by changing the config file and reloading, without needing a developer to edit and redeploy the Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e145b23",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "An application uses the following logging.ini file for its configuration. Given this configuration, what happens when logging.getLogger(\"app.tasks\").info(\"Task started\") is called?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980c33d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "[loggers]\n",
    "keys=root,tasks\n",
    " \n",
    "[handlers]\n",
    "keys=console,file\n",
    " \n",
    "[formatters]\n",
    "keys=simple\n",
    " \n",
    "[logger_root]\n",
    "level=WARNING\n",
    "handlers=console\n",
    " \n",
    "[logger_tasks]\n",
    "level=INFO\n",
    "handlers=file\n",
    "qualname=app.tasks\n",
    "propagate=0\n",
    " \n",
    "[handler_console]\n",
    "class=StreamHandler\n",
    "level=WARNING\n",
    "formatter=simple\n",
    "args=(sys.stdout,)\n",
    " \n",
    "[handler_file]\n",
    "class=FileHandler\n",
    "level=DEBUG\n",
    "formatter=simple\n",
    "args=('task.log', 'w')\n",
    " \n",
    "[formatter_simple]\n",
    "format=%(name)s:%(levelname)s:%(message)s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fb8266",
   "metadata": {},
   "source": [
    "The message is written only to the task.log file.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. The app.tasks logger is configured with a level of INFO, so it accepts the message. It sends the message to its configured handler, file, which writes to task.log. The propagate=0 setting stops the message from continuing up to the root logger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd459f0",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "When using logging.config.dictConfig, what is the purpose of the top-level disable_existing_loggers key?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de74e0c",
   "metadata": {},
   "source": [
    "\n",
    "If True (the default), it disables all loggers that existed before the dictConfig call, unless they or their ancestors are explicitly named in the new configuration. This helps ensure a clean, predictable logging setup.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the precise function. It's a safety measure to prevent old, imperatively configured loggers (e.g., from a third-party library that was imported earlier) from interfering with the new, declarative setup. Setting it to False is necessary if you intend to merge the new configuration with existing ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a79e1f",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "An application needs to dynamically set the log level based on an environment variable. If DEBUG_MODE is active, all loggers should be set to DEBUG; otherwise, they should use their specified levels. What is the output of this script?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23eb88e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Service A detailed trace.\n",
      "Service B started.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import logging.config\n",
    " \n",
    "def get_log_config(is_debug):\n",
    "    config = {\n",
    "        \"version\": 1,\n",
    "        \"disable_existing_loggers\": False,\n",
    "        \"formatters\": {\"simple\": {\"format\": \"%(message)s\"}},\n",
    "        \"handlers\": {\n",
    "            \"console\": {\n",
    "                \"class\": \"logging.StreamHandler\",\n",
    "                \"level\": \"INFO\",\n",
    "                \"formatter\": \"simple\"\n",
    "            }\n",
    "        },\n",
    "        \"loggers\": {\n",
    "            \"service_a\": {\"level\": \"WARNING\", \"handlers\": [\"console\"]},\n",
    "            \"service_b\": {\"level\": \"ERROR\", \"handlers\": [\"console\"]}\n",
    "        }\n",
    "    }\n",
    "    if is_debug:\n",
    "        for logger_name in config[\"loggers\"]:\n",
    "            config[\"loggers\"][logger_name][\"level\"] = \"DEBUG\"\n",
    "        config[\"handlers\"][\"console\"][\"level\"] = \"DEBUG\"\n",
    " \n",
    "    return config\n",
    " \n",
    "logging.config.dictConfig(get_log_config(is_debug=True))\n",
    "logging.getLogger(\"service_a\").debug(\"Service A detailed trace.\")\n",
    "logging.getLogger(\"service_b\").info(\"Service B started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eff8639",
   "metadata": {},
   "source": [
    "Correct. When is_debug=True, the configuration is modified at runtime. The levels for service_a and service_b loggers are both changed to DEBUG, and the console handler's level is also changed to DEBUG. Consequently, both the DEBUG message from service_a and the INFO message from service_b are accepted by their respective loggers and by the handler, and are printed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7cbeda",
   "metadata": {},
   "source": [
    "Question 5:\n",
    "Which configuration format, INI-style (fileConfig) or Dictionary-style (dictConfig), offers more flexibility for complex and dynamic logging setups, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb2b85b",
   "metadata": {},
   "source": [
    "Dictionary-style, because it can natively represent complex data types (lists, nested dictionaries), can be constructed programmatically, and easily serialized/deserialized from expressive formats like JSON or YAML.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. The dictionary schema is superior for complex setups. It allows for non-string values (e.g., for custom filters or handlers that take complex arguments), can be built and modified in Python code before being applied, and maps directly to modern data interchange formats like JSON and YAML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66737821",
   "metadata": {},
   "source": [
    "## File System Interaction and I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84660a9",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "In modern Python (3.6+), why is using the pathlib module generally preferred over the older os.path module for filesystem path manipulations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53acf6b",
   "metadata": {},
   "source": [
    "\n",
    "pathlib provides an object-oriented API where paths are objects with methods, making code more readable and expressive compared to the string-based functional approach of os.path.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the core reason. With pathlib, a path is an object, not just a string. You can call methods directly on it (e.g., p.exists(), p.is_dir()) and use operators like / for joining paths. This leads to cleaner, more intuitive, and less error-prone code than calling separate functions like os.path.exists(p) or os.path.join(p, \"dir\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa271009",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "A DevOps script needs to construct a path to a deployment artifact and print its components. Analyze the following script. What is the exact output of this script?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22ea3381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: app-v2.1.tar.gz\n",
      "Parent: artifacts\n",
      "Stem: app-v2.1.tar\n",
      "Suffix: .gz\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    " \n",
    "base_dir = Path(\"/opt/deployments/artifacts\")\n",
    "artifact_file = \"app-v2.1.tar.gz\"\n",
    " \n",
    "full_path = base_dir / artifact_file\n",
    " \n",
    "print(f\"Name: {full_path.name}\")\n",
    "print(f\"Parent: {full_path.parent.name}\")\n",
    "print(f\"Stem: {full_path.stem}\")\n",
    "print(f\"Suffix: {full_path.suffix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60fdfb4",
   "metadata": {},
   "source": [
    "Correct. .name is the full final path component: app-v2.1.tar.gz. .parent is the path to the directory containing the file (/opt/deployments/artifacts), and .parent.name is its final component: artifacts. .suffix is only the final dot-separated part: .gz. .stem is the name without the final suffix: app-v2.1.tar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6508d5e",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "A developer writes a function to create a unique lock file for a process. The function should only succeed if the lock file does not already exist, to prevent multiple instances of a process from running. The function, however, never prints \"Lock file already exists.\"; instead, it overwrites any existing file with the new pid. Why does this code not behave as intended?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ca8f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    " \n",
    "def create_lock_file(pid):\n",
    "    lock_path = Path(f\"process_{pid}.lock\")\n",
    "    try:\n",
    "        with lock_path.open(mode='w', encoding='utf-8') as f:\n",
    "            f.write(str(os.getpid()))\n",
    "        print(\"Lock file created successfully.\")\n",
    "        return True\n",
    "    except FileExistsError:\n",
    "        print(\"Lock file already exists.\")\n",
    "        return False\n",
    " \n",
    "# Simulate that the file already exists\n",
    "Path(\"process_123.lock\").touch()\n",
    " \n",
    "create_lock_file(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6d4bea",
   "metadata": {},
   "source": [
    "\n",
    "Using mode='w' overwrites an existing file or creates a new one; it never raises a FileExistsError. To achieve the desired behavior, mode='x' (exclusive creation) should be used.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the bug. The 'w' mode's behavior is to truncate and overwrite any existing file. It will never raise a FileExistsError. The 'x' mode is designed specifically for this \"create only if it does not exist\" use case and will raise FileExistsError if the path already exists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889c8035",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "You need to write a Python script to parse a 10 GB log file and count the occurrences of the word \"ERROR\". Which method of reading the file is most appropriate for this task to ensure the script does not consume an excessive amount of memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715e2169",
   "metadata": {},
   "source": [
    "\n",
    "for line in file:\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. Iterating directly over the file object (for line in file:) is the most memory-efficient method. Python reads the file line-by-line, loading only one line into memory at a time. This allows the script to process files of any size with a minimal and constant memory footprint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad030b3",
   "metadata": {},
   "source": [
    "Question 5:\n",
    "A developer is writing a list of server hostnames to a configuration file. Analyze the following code. What will be the final content of the servers.conf file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d44a0626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    " \n",
    "servers = [\"web-01\\n\", \"db-01\\n\", \"cache-01\\n\"]\n",
    "config_file = Path(\"servers.conf\")\n",
    " \n",
    "with config_file.open(mode='w', encoding='utf-8') as f:\n",
    "    f.writelines(servers)\n",
    " \n",
    "with config_file.open(mode='a', encoding='utf-8') as f:\n",
    "    f.write(\"monitor-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ec0507",
   "metadata": {},
   "source": [
    "web-01\n",
    "db-01\n",
    "cache-01\n",
    "monitor-01\n",
    "Correct\n",
    "\n",
    "Correct. The first with block opens the file in write (w) mode, truncating it. f.writelines(servers) writes each string from the list to the file. Since each string in the servers list already contains a newline character (\\n), each server is written on its own line. The second with block opens the file in append (a) mode and f.write(\"monitor-01\") adds \"monitor-01\" immediately after the last character of the existing content, which was the newline after \"cache-01\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0e1b0d",
   "metadata": {},
   "source": [
    "## Text Processing with Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34b6fbf",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "A DevOps engineer needs to parse log files where lines can be in slightly different formats, such as ERROR: [auth-service] - Login failed or WARN: [db-service] - Connection slow. Why is using the re module generally more robust for this task than using string methods like str.split() and str.find()?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c09e1d5",
   "metadata": {},
   "source": [
    "\n",
    "The re module can define flexible patterns that accommodate variations in whitespace, optional components, and different keywords, whereas string methods rely on fixed, literal substrings.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the key advantage of regex. A single pattern like r\"(ERROR|WARN):\\s+\\[(.*?)\\]\" can handle multiple log levels, variable whitespace (\\s+), and extract the service name ((.*?)) reliably. Achieving this with str.split() and str.find() would require complex, brittle, and hard-to-maintain if/else logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f0f0b",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "A script is written to extract all IP addresses and status codes from a web server access log. What is the output of this script?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e394b484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPs: ['192.168.1.10', '10.0.0.5', '172.16.0.2']\n",
      "Codes: ['200', '201', '404']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "log_data = \"\"\"\n",
    "192.168.1.10 - - [10/Mar/2023:13:55:36 +0000] \"GET /api/v1/users HTTP/1.1\" 200 512\n",
    "10.0.0.5 - - [10/Mar/2023:13:56:12 +0000] \"POST /api/v1/data HTTP/1.1\" 201 1024\n",
    "172.16.0.2 - - [10/Mar/2023:13:57:01 +0000] \"GET /static/style.css HTTP/1.1\" 404 2326\n",
    "\"\"\"\n",
    " \n",
    "ip_pattern = r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\"\n",
    "status_code_pattern = r'\"\\s(\\d{3})\\s'\n",
    " \n",
    "ips = re.findall(ip_pattern, log_data)\n",
    "codes = re.findall(status_code_pattern, log_data)\n",
    " \n",
    "print(f\"IPs: {ips}\")\n",
    "print(f\"Codes: {codes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cce19cc",
   "metadata": {},
   "source": [
    "Correct. The ip_pattern has no capturing groups, so re.findall returns a list of all full strings that match the pattern. The status_code_pattern has one capturing group (\\d{3}). re.findall therefore returns a list containing only the strings captured by that group, which are the 3-digit status codes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f98ed",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "A developer wants to extract the content inside an XML-like tag. Their code is returning a much larger string than expected, instead of extracting only the first sender <sender>user1</sender>. What is the cause of the bug?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c590471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "text = \"<message><sender>user1</sender><content>hello</content></message><message><sender>user2</sender><content>hi</content></message>\"\n",
    " \n",
    "pattern = r\"<sender>.*</sender>\"\n",
    " \n",
    "match = re.search(pattern, text)\n",
    " \n",
    "if match:\n",
    "    print(match.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c7dfd9",
   "metadata": {},
   "source": [
    "The * quantifier is greedy by default. It makes .* match as many characters as possible, causing it to match from the first <sender> tag to the very last </sender> tag in the string.\n",
    "\n",
    "Correct\n",
    "\n",
    "This is the classic greedy vs. non-greedy problem. The * quantifier will match everything until it finds the final possible . The fix is to use a non-greedy (or lazy) quantifier, *?, so the pattern becomes r\".*?\", which stops at the first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad16677d",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "A script is designed to parse key-value pairs from service configuration lines. It uses named capturing groups to make the extracted data easier to work with. What is the output of this script?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8cef4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nginx is running on port 443 using https\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "config_line = \"service=nginx; port=443; protocol=https;\"\n",
    "pattern = r\"service=(?P<name>\\w+);\\s*port=(?P<port>\\d+);\\s*protocol=(?P<proto>\\w+);\"\n",
    " \n",
    "match = re.search(pattern, config_line)\n",
    " \n",
    "if match:\n",
    "    data = match.groupdict()\n",
    "    print(f\"{data['name']} is running on port {data['port']} using {data['proto']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21285bc0",
   "metadata": {},
   "source": [
    "Correct. The regex pattern correctly uses named capturing groups (?P...) to capture nginx, 443, and https. The re.search() finds a match, and match.groupdict() returns a dictionary: {'name': 'nginx', 'port': '443', 'proto': 'https'}. The f-string then correctly formats this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f8b483",
   "metadata": {},
   "source": [
    "Question 5:\n",
    "What is the main advantage of using re.finditer() over re.findall() when you need to process a very large number of matches in a large text file?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb1c8e6",
   "metadata": {},
   "source": [
    "re.finditer() returns an iterator that yields match objects one by one, making it highly memory-efficient. re.findall() constructs a full list of all matches in memory at once.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the key difference. For a file with millions of matches, re.findall() could consume a huge amount of memory by building a giant list. re.finditer() is a lazy approach; it finds one match, yields it for processing, and only then moves on to find the next one, keeping memory usage low and constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e505095",
   "metadata": {},
   "source": [
    "## Working with Data Formats (JSON, YAML, CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b43cc5",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "A DevOps team is deciding on a configuration file format for a new application. The configuration will be maintained by engineers of varying technical backgrounds and needs to support comments for documentation. Between JSON and YAML, which is the better choice and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cf5233",
   "metadata": {},
   "source": [
    "YAML, because its syntax is designed for human readability, using indentation for structure and allowing comments (#), which makes it easier to document and manage complex configuration files.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. YAML's primary design goal is human-friendliness. The ability to add comments, the minimal syntax (no mandatory quotes or braces), and the clear block structure make it an excellent choice for configuration files that humans need to read, understand, and edit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ee5eae",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "A script needs to parse a JSON string received from a monitoring API and extract specific details. What is the output of this script?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f22271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc-web-01 CPU: 15.5%, Is_Prod: True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    " \n",
    "api_response = \"\"\"\n",
    "{\n",
    "    \"service_id\": \"svc-web-01\",\n",
    "    \"status\": \"HEALTHY\",\n",
    "    \"metrics\": {\n",
    "        \"cpu_percent\": 15.5,\n",
    "        \"memory_mb\": 1024\n",
    "    },\n",
    "    \"tags\": [\"prod\", \"frontend\"],\n",
    "    \"enabled\": true\n",
    "}\n",
    "\"\"\"\n",
    " \n",
    "try:\n",
    "    data = json.loads(api_response)\n",
    "    service = data.get(\"service_id\")\n",
    "    cpu = data[\"metrics\"][\"cpu_percent\"]\n",
    "    is_prod = \"prod\" in data.get(\"tags\", [])\n",
    "    print(f\"{service} CPU: {cpu}%, Is_Prod: {is_prod}\")\n",
    "except (json.JSONDecodeError, KeyError) as e:\n",
    "    print(f\"Error parsing data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2e9250",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "A developer is writing a Python dictionary to a CSV file. The script runs without error, but the output CSV file is missing the header row. What is the cause of the missing header row in status_report.csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b49c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    " \n",
    "report_data = [\n",
    "    {'hostname': 'db-master-01', 'status': 'OK', 'region': 'us-east-1'},\n",
    "    {'hostname': 'web-worker-05', 'status': 'FAIL', 'region': 'eu-west-1'}\n",
    "]\n",
    "field_order = ['hostname', 'region', 'status']\n",
    "output_path = Path(\"status_report.csv\")\n",
    " \n",
    "# Buggy Code\n",
    "with output_path.open('w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=field_order)\n",
    "    # The header row is not being written\n",
    "    writer.writerows(report_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5130e8",
   "metadata": {},
   "source": [
    "csv.DictWriter does not write headers automatically. The developer must explicitly call the writer.writeheader() method before writing the data rows.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the precise bug. Unlike csv.writer, which treats all rows as data, csv.DictWriter makes a distinction between the header and data rows. To write the header (based on the fieldnames provided during instantiation), you must make an explicit call to writer.writeheader()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1eb065",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "Your script needs to read a simple CSV file where some fields might be empty. What is the most robust way to read this data into a format that allows you to access columns by name (row['hostname'])?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d440cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# servers.csv\n",
    "hostname,ip_address,status\n",
    "web-01,1.1.1.1,online\n",
    "db-01,,offline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc20bf98",
   "metadata": {},
   "source": [
    "Use the csv.DictReader object, which automatically uses the first row as headers and returns each subsequent row as a dictionary.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. csv.DictReader is designed for exactly this purpose. It reads the header row to determine the keys and then yields each data row as a dictionary (e.g., {'hostname': 'db-01', 'ip_address': '', 'status': 'offline'}). This allows for robust, name-based access to columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d165d",
   "metadata": {},
   "source": [
    "Question 5:\n",
    "A script serializes a Python dictionary to a JSON string for an API payload. The indent and sort_keys parameters are used to ensure the output is consistent and readable. What is the output printed to the console?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec815eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"action\": \"deploy\",\n",
      "  \"commit_hash\": \"a1b2c3d4\",\n",
      "  \"environment\": \"staging\",\n",
      "  \"user\": \"ci-bot\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    " \n",
    "payload = {\n",
    "    \"user\": \"ci-bot\",\n",
    "    \"action\": \"deploy\",\n",
    "    \"environment\": \"staging\",\n",
    "    \"commit_hash\": \"a1b2c3d4\"\n",
    "}\n",
    " \n",
    "json_string = json.dumps(payload, indent=2, sort_keys=True)\n",
    "print(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bb6cdb",
   "metadata": {},
   "source": [
    "Correct. The indent=2 argument formats the JSON with newlines and a two-space indent. The sort_keys=True argument ensures that the keys in the output object are sorted alphabetically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e143b4d",
   "metadata": {},
   "source": [
    "## Environment Variables and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178a2015",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "A DevOps team manages a Python application that is deployed to three different environments: development, staging, and production. Each environment connects to a different database. What is the most robust and conventional method to manage the database connection string for this application?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f55159",
   "metadata": {},
   "source": [
    "Store the connection string in an environment variable (for example, DATABASE_URL) that is set differently in each environment's runtime.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the standard and best practice. It decouples the configuration from the application code, allowing operators to manage connection details per environment without touching the source. It also prevents secrets from being committed to version control."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da719115",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "A script is written to configure a connection to a monitoring service. It needs to read an endpoint URL and an optional timeout value from environment variables. What is the output of this script?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e52d7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Endpoint not configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    " \n",
    "# Assume the following environment variable is set before the script runs:\n",
    "# export SERVICE_ENDPOINT=\"https://monitor.example.com/api\"\n",
    " \n",
    "def get_service_config():\n",
    "    endpoint = os.getenv(\"SERVICE_ENDPOINT\")\n",
    "    timeout_str = os.getenv(\"TIMEOUT_SECONDS\", \"10\") # Default is a string\n",
    " \n",
    "    if not endpoint:\n",
    "        return \"Error: Endpoint not configured.\"\n",
    " \n",
    "    return f\"Connecting to {endpoint} with a timeout of {timeout_str}s.\"\n",
    " \n",
    "print(get_service_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba49391f",
   "metadata": {},
   "source": [
    "Connecting to https://monitor.example.com/api with a timeout of 10s.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. os.getenv(\"SERVICE_ENDPOINT\") successfully retrieves the URL. The TIMEOUT_SECONDS variable is not set, so os.getenv returns the provided default value, which is the string \"10\". The function then constructs and returns the formatted string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d0bf09",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "A developer writes a script to configure the number of worker processes based on an environment variable. The script fails with a TypeError. What is the fundamental cause of the TypeError?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc3919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "# Assume this is set: export WORKER_COUNT=\"4\"\n",
    " \n",
    "def start_workers():\n",
    "    num_workers = os.getenv(\"WORKER_COUNT\")\n",
    "    if num_workers:\n",
    "        print(f\"Starting {num_workers} workers...\")\n",
    "        for i in range(num_workers):\n",
    "            print(f\"  - Worker {i+1} started.\")\n",
    "    else:\n",
    "        print(\"WORKER_COUNT not set, starting 1 worker.\")\n",
    " \n",
    "start_workers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba4222f",
   "metadata": {},
   "source": [
    "The os.getenv() function always returns a string. The range() function requires an integer argument, but it received the string \"4\".\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the core issue. All values retrieved from environment variables are strings. The code attempts range(\"4\"), which is a TypeError. The fix is to explicitly convert the value to an integer: for i in range(int(num_workers)):."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a958ae",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "When should a developer choose to access an environment variable using os.environ['MY_VAR'] versus os.getenv('MY_VAR')?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84966113",
   "metadata": {},
   "source": [
    "\n",
    "os.environ[] should be used for mandatory configuration that is essential for the application to run. os.getenv() should be used for optional configuration where a None or default value is acceptable.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the fundamental design choice. Using os.environ[] for a required variable like a database host will cause the application to fail fast with a KeyError if it's not configured, which is good. Using os.getenv() for an optional setting like a LOG_LEVEL allows the program to proceed with a sensible default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6795b419",
   "metadata": {},
   "source": [
    "Question 5:\n",
    "A developer is using a .env file for local development to avoid setting environment variables in their shell. They want to test how the override parameter of load_dotenv works. What is the output of this script?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28530bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    " \n",
    "# Assume a file named \".env\" exists with the content:\n",
    "# DATABASE_URL=postgres://user:pass@dotenv_host/db\n",
    " \n",
    "# Step 1: Set a variable in the script's environment first\n",
    "os.environ[\"DATABASE_URL\"] = \"postgres://user:pass@shell_host/db\"\n",
    " \n",
    "# Step 2: Load .env without override\n",
    "load_dotenv(override=False)\n",
    "print(f\"Without override: {os.getenv('DATABASE_URL')}\")\n",
    " \n",
    "# Step 3: Load .env WITH override\n",
    "load_dotenv(override=True)\n",
    "print(f\"With override: {os.getenv('DATABASE_URL')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89564444",
   "metadata": {},
   "source": [
    "Without override: postgres://user:pass@shell_host/db\n",
    "With override: postgres://user:pass@dotenv_host/db\n",
    "Correct\n",
    "\n",
    "Correct. In Step 2, load_dotenv(override=False) is called. Since DATABASE_URL already exists in the environment (set in Step 1), it is not overwritten. os.getenv returns the existing \"shell_host\" value. In Step 3, load_dotenv(override=True) is called. This forces the function to load values from the .env file even if they already exist in the environment. os.getenv now returns the \"dotenv_host\" value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb70b0e",
   "metadata": {},
   "source": [
    "## Filesystem and Directory Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03b0723",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "A backup script needs to create a directory structure like /mnt/backups/2023-10-27/. If the parent directories /mnt/backups/ and /mnt/backups/2023-10-27/ might not exist, which method is the most robust and Pythonic way to create this structure?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf6c912",
   "metadata": {},
   "source": [
    "Path(\"/mnt/backups/2023-10-27/\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the ideal method. parents=True ensures that all necessary parent directories (/mnt/backups) are created. exist_ok=True ensures that no error is raised if the directory already exists. This makes the operation idempotent and highly robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c81b5d",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "A script needs to clean up a temporary directory by deleting all .tmp files but leaving other files untouched. The directory contains a mix of files and one subdirectory. What is printed to the console after the following script executes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca9063c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['archive', 'config.ini']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    " \n",
    "# --- Setup for demonstration purposes ---\n",
    "base_dir = Path(\"test_cleanup\")\n",
    "if base_dir.exists():\n",
    "    shutil.rmtree(base_dir)\n",
    "base_dir.mkdir()\n",
    "(base_dir / \"data1.tmp\").touch()\n",
    "(base_dir / \"data2.tmp\").touch()\n",
    "(base_dir / \"config.ini\").touch()\n",
    "(base_dir / \"archive\").mkdir()\n",
    " \n",
    "# --- Core logic ---\n",
    "for item in base_dir.iterdir():\n",
    "    if item.is_file() and item.suffix == \".tmp\":\n",
    "        item.unlink()\n",
    " \n",
    "# --- Verification ---\n",
    "remaining_items = sorted([p.name for p in base_dir.iterdir()])\n",
    "print(remaining_items)\n",
    " \n",
    "# --- Cleanup after demonstration ---\n",
    "shutil.rmtree(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1f509c",
   "metadata": {},
   "source": [
    "Correct. The for loop iterates through the contents of test_cleanup. data1.tmp: is_file() is true and suffix is .tmp, so item.unlink() is called. data2.tmp: is_file() is true and suffix is .tmp, so item.unlink() is called. config.ini: is_file() is true but suffix is .ini, so it is skipped. archive: is_file() is false, so it is skipped.The remaining items are the directory archive and the file config.ini."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5bf0d0",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "A developer writes a script to remove a directory named old_build. The script works when the directory is empty but fails with an OSError when the directory contains files. Why does the p.rmdir() call fail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef588be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    " \n",
    "# --- Setup for demonstration ---\n",
    "p = Path(\"old_build\")\n",
    "p.mkdir(exist_ok=True)\n",
    "(p / \"app.bin\").touch()\n",
    " \n",
    "# --- Core logic ---\n",
    "try:\n",
    "    # This line fails if \"old_build\" is not empty\n",
    "    p.rmdir()\n",
    "    print(\"Directory 'old_build' removed.\")\n",
    "except OSError as e:\n",
    "    print(f\"Error: {e}\")\n",
    " \n",
    "# --- Cleanup after demonstration ---\n",
    "if p.exists():\n",
    "    shutil.rmtree(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b5f49d",
   "metadata": {},
   "source": [
    "The rmdir() method can only remove empty directories. To recursively delete a directory and all its contents, shutil.rmtree() must be used.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the fundamental difference between the two functions. Path.rmdir() (and os.rmdir()) is a safe-by-default operation that will only succeed on an empty directory. For a forceful, recursive deletion, the shutil.rmtree() function is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df649b1",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "A script needs to download a large file, process it, and then ensure the downloaded file is deleted, even if an error occurs during processing. The file does not need to remain saved after processing. What is the most reliable and Pythonic way to manage the lifecycle of this downloaded file?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17725e5",
   "metadata": {},
   "source": [
    "\n",
    "Use tempfile.NamedTemporaryFile(). Write the downloaded content to this file, process it, and allow the context manager to automatically delete the file upon exiting the with block.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the ideal solution. NamedTemporaryFile creates a file with a unique, non-colliding name in a secure temporary location. The with statement guarantees that the file is automatically cleaned up (since delete=True is the default) when the block is exited, whether normally or due to an exception."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34eb2e",
   "metadata": {},
   "source": [
    "Question 5:\n",
    "A data processing workflow creates multiple intermediate files within a temporary directory. The script uses a context manager to handle the cleanup. What will the output of this script be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ab6e92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: C:\\Users\\SHUBHE~1\\AppData\\Local\\Temp\\tmpmtlhlgl6\n",
      "Directory C:\\Users\\SHUBHE~1\\AppData\\Local\\Temp\\tmpmtlhlgl6 exists after with-block: False\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    " \n",
    "# --- Action ---\n",
    "final_path_str = \"\"\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    print(f\"Created directory: {temp_dir}\")\n",
    "    temp_path_obj = Path(temp_dir)\n",
    " \n",
    "    (temp_path_obj / \"step1.dat\").touch()\n",
    "    (temp_path_obj / \"step2.dat\").touch()\n",
    " \n",
    "    final_path_str = str(temp_path_obj)\n",
    " \n",
    "# --- Verification ---\n",
    "print(f\"Directory {final_path_str} exists after with-block: {Path(final_path_str).exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94a48cd",
   "metadata": {},
   "source": [
    "Correct. The with tempfile.TemporaryDirectory() ... block creates a temporary directory. All operations inside the block succeed. Upon exiting the with block, the context manager's cleanup logic is triggered, which recursively deletes the temporary directory and all files inside it. Therefore, the final check Path(...).exists() will evaluate to False."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28ca998",
   "metadata": {},
   "source": [
    "## Running and Managing Subprocesses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe32737",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "A DevOps script needs to delete a Docker container based on an ID provided by a user. Why is subprocess.run(['docker', 'rm', user_provided_id]) significantly safer than os.system(f\"docker rm {user_provided_id}\")?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede0b350",
   "metadata": {},
   "source": [
    "\n",
    "subprocess.run() (with default shell=False) passes arguments as a list of tokens, preventing the shell from interpreting metacharacters in the input. This mitigates shell injection vulnerabilities.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the crucial security benefit. If a user provided an ID like my-container; rm -rf /, os.system() would execute both commands. subprocess.run() passes \"my-container; rm -rf /\" as a single, literal argument to docker rm, which would safely fail without executing the malicious second command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32471f0",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "A script is used to check the current version of an installed command-line tool, like pip. What is the most likely output of this script in a standard Python environment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f3edfde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 25.1 from C:\\Users\\Shubhesh Swain\\anaconda3\\Lib\\site-packages\\pip (python 3.13)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    " \n",
    "try:\n",
    "    command = [sys.executable, \"-m\", \"pip\", \"--version\"]\n",
    "    result = subprocess.run(\n",
    "        command,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        check=True\n",
    "    )\n",
    "    version_info = result.stdout.strip()\n",
    "    print(version_info)\n",
    "except (subprocess.CalledProcessError, FileNotFoundError) as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf0c5af",
   "metadata": {},
   "source": [
    "\n",
    "A string containing the pip version information, such as pip 24.3.1 from /path/to/lib/python3.12/site-packages/pip (python 3.12).\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. The command correctly invokes the pip module to get its version. subprocess.run captures the output, text=True decodes it to a string, and strip() removes leading/trailing whitespace. The final print displays this captured string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234e3bb5",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "A developer writes a script to archive a directory using the tar command. The script seems to run, but the archive is not created when an invalid source directory is given. The script, however, does not report an error. Why does the script proceed and print \"Archive process finished...\" even though the tar command failed with a non-zero exit code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fc4df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    " \n",
    "source_dir = \"non_existent_data/\"\n",
    "archive_file = \"backup.tar.gz\"\n",
    "command = [\"tar\", \"-czf\", archive_file, source_dir]\n",
    " \n",
    "result = subprocess.run(command)\n",
    " \n",
    "print(\"Archive process finished. Continuing with next steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4602a0",
   "metadata": {},
   "source": [
    "\n",
    "By default, subprocess.run does not raise an exception for a non-zero exit code. The failure is silent unless the result.returncode is manually checked or check=True is used.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the fundamental issue. The default behavior of subprocess.run is to run the command and report the outcome in the CompletedProcess object, but it does not treat a non-zero exit code as a Python exception. To make it fail loudly, the developer must add check=True to the call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8eb2bf",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "A script attempts to delete a Kubernetes secret. If the secret does not exist, the kubectl command will fail with a non-zero exit code and an error message on stderr. The script is designed to handle this specific case gracefully. Assuming the kubectl command fails because the secret does not exist, what is the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818765e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    " \n",
    "secret_name = \"this-is-a-non-existent-secret\"\n",
    "command = [\"kubectl\", \"delete\", \"secret\", secret_name]\n",
    " \n",
    "try:\n",
    "    subprocess.run(\n",
    "        command,\n",
    "        check=True,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    print(f\"Secret '{secret_name}' deleted successfully.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    if \"NotFound\" in e.stderr:\n",
    "        print(f\"Info: Secret '{secret_name}' did not exist. No action taken.\")\n",
    "    else:\n",
    "        print(f\"Error deleting secret: {e.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b453bb18",
   "metadata": {},
   "source": [
    "\n",
    "Info: Secret 'this-is-a-non-existent-secret' did not exist. No action taken.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. subprocess.run with check=True raises a CalledProcessError. The except block catches the error. The error message from kubectl for a missing resource includes the word \"NotFound\". The if \"NotFound\" in e.stderr: condition evaluates to True, and the corresponding informational message is printed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7de189",
   "metadata": {},
   "source": [
    "Question 5:\n",
    "What is the key difference between the subprocess.CalledProcessError and FileNotFoundError exceptions when using subprocess.run()?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630c465c",
   "metadata": {},
   "source": [
    "\n",
    "FileNotFoundError is raised by subprocess.run() if the executable itself cannot be found in the system's PATH. CalledProcessError is raised (if check=True) when the command is found and runs, but exits with a non-zero status code.\n",
    "\n",
    "Correct\n",
    "\n",
    "This correctly distinguishes the two failure modes. FileNotFoundError means the command could not even be started. CalledProcessError means the command started, ran, and then reported an error upon exiting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c33b90",
   "metadata": {},
   "source": [
    "Question 6:\n",
    "A developer needs to pass a command with multiple arguments, including one with a space, to subprocess.run().\n",
    "\n",
    "Command to run: aws s3 cp my-local-file.txt \"s3://my-bucket/target path/file.txt\"\n",
    "\n",
    "Which list correctly and safely represents this command for subprocess.run()?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e4dca2",
   "metadata": {},
   "source": [
    "\n",
    "['aws', 's3', 'cp', 'my-local-file.txt', 's3://my-bucket/target path/file.txt']\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the correct representation. Each distinct argument to the command, as it would be separated by spaces in a shell (while respecting quotes), becomes a separate string element in the list. The argument containing a space is passed as a single list element, ensuring it is treated as one argument by the aws command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2414e49f",
   "metadata": {},
   "source": [
    "## Making and Inspecting HTTP Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47395883",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "A developer needs to interact with a modern REST API that returns JSON data. Why is the requests library generally preferred over Python's built-in urllib.request module for this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf41e71a",
   "metadata": {},
   "source": [
    "\n",
    "requests provides a simpler, more human-friendly API, with features like a built-in JSON decoder (.json()) and automatic handling of query strings, which significantly reduces the amount of boilerplate code.\n",
    "\n",
    "Correct\n",
    "\n",
    "This is the primary reason. The requests library was designed to be intuitive. Tasks that are complex in urllib (like sending a JSON payload, handling authentication, or managing sessions) are often single-line operations in requests, making the code cleaner and easier to maintain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c4bab7",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "A script needs to search a package repository API for packages matching specific criteria. What is the most likely URL that will be printed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe122a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    " \n",
    "api_url = \"https://api.pypackage.org/search\"\n",
    "query_data = {\n",
    "    \"name\": \"devops-automation\",\n",
    "    \"min_version\": \"2.1\",\n",
    "    \"license\": \"MIT\"\n",
    "}\n",
    " \n",
    "response = requests.get(api_url, params=query_data, timeout=5)\n",
    " \n",
    "print(response.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bc7e8f",
   "metadata": {},
   "source": [
    "\n",
    "https://api.pypackage.org/search?name=devops-automation&min_version=2.1&license=MIT\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct.The requests library correctly serializes the params dictionary into a URL-encoded query string, joining key-value pairs with = and separating pairs with &."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0407fc79",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "A developer is trying to create a new user by sending a JSON payload to an API endpoint. The server responds with an error indicating a malformed request, even though the Python dictionary seems correct. What is the most likely cause of the server error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f3b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    " \n",
    "api_url = \"https://api.example.com/v1/users\"\n",
    "new_user_data = {\n",
    "    \"username\": \"cicd_bot\",\n",
    "    \"permissions\": [\"read\", \"deploy\"]\n",
    "}\n",
    " \n",
    "response = requests.post(api_url, data=new_user_data)\n",
    " \n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b379849",
   "metadata": {},
   "source": [
    "The data parameter form-encodes the dictionary (username=cicd_bot&...). To send a JSON payload, the json parameter should be used instead (json=new_user_data).\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the precise bug. Using data= causes requests to send the payload as application/x-www-form-urlencoded. Modern APIs typically expect application/json. The json parameter is the correct way to handle this; it automatically serializes the dictionary to a JSON string and sets the Content-Type header to application/json."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f02735",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "A developer is writing a script to fetch an image from a URL. The script runs, but the saved logo.png file is corrupted and cannot be opened. What is the cause of the corrupted image file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f309d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    " \n",
    "image_url = \"https://httpbin.org/image/png\"\n",
    "response = requests.get(image_url)\n",
    " \n",
    "if response.status_code == 200:\n",
    "    with open(\"logo.png\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c5d08a",
   "metadata": {},
   "source": [
    "Image data is binary. The script uses response.text, which tries to decode the binary data as text (often UTF-8), corrupting it. It should use response.content and open the file in binary write mode ('wb').\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the fundamental error. response.text is for text-based content and applies a character encoding. response.content provides the raw, unmodified bytes of the response body. For non-text data like images, PDFs, or zip files, one must use response.content and write it to a file opened in binary mode ('wb')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af58a92",
   "metadata": {},
   "source": [
    "Question 5:\n",
    "What is the key difference between the response.text and response.content attributes of a requests Response object?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af8dc29",
   "metadata": {},
   "source": [
    "\n",
    "response.text returns the response body as a decoded string, while response.content returns the raw response body as bytes.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. response.text takes the raw bytes and decodes them into a Python string, usually based on the Content-Type header or a default encoding (like UTF-8). response.content provides the raw, unprocessed bytes, which is essential for handling non-textual data like images or compressed files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d9904",
   "metadata": {},
   "source": [
    "## Robust API Interaction: Authentication and Error Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0198362",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "When automating interactions with an API, what is the primary purpose of calling response.raise_for_status() immediately after receiving a response?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2529fc",
   "metadata": {},
   "source": [
    "It checks the response's status code and raises an HTTPError exception if the code indicates a client error (4xx) or server error (5xx), allowing for centralized error handling.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the core function of raise_for_status(). It turns failed HTTP responses into Python exceptions, allowing you to use a try...except block to handle all kinds of failures cleanly instead of writing if/else statements to check the status code manually. This promotes the \"Easier to Ask for Forgiveness than Permission\" (EAFP) coding style."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e89dca",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "A script attempts to fetch data from a protected API endpoint that requires a Bearer Token for authentication. If the API_TOKEN is expired and the server returns a 401 Unauthorized status code, what will the script print?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed3fc4ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='api.cloudservice.com', port=443): Max retries exceeded with url: /v1/data (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000195A6891090>: Failed to resolve 'api.cloudservice.com' ([Errno 11001] getaddrinfo failed)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    199\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    201\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[0;32m    202\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[0;32m    203\u001b[0m     )\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, socket\u001b[38;5;241m.\u001b[39mSOCK_STREAM):\n\u001b[0;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:977\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    976\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 977\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m _socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, \u001b[38;5;28mtype\u001b[39m, proto, flags):\n\u001b[0;32m    978\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1093\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:704\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    703\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 704\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[0;32m    705\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x00000195A6891090>: Failed to resolve 'api.cloudservice.com' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    679\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    842\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    843\u001b[0m )\n\u001b[0;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='api.cloudservice.com', port=443): Max retries exceeded with url: /v1/data (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000195A6891090>: Failed to resolve 'api.cloudservice.com' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 16\u001b[0m\n\u001b[0;32m     10\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m }\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 16\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(api_url, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     17\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Assume the API returns: {\"data\": [\"item1\", \"item2\"]}\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='api.cloudservice.com', port=443): Max retries exceeded with url: /v1/data (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000195A6891090>: Failed to resolve 'api.cloudservice.com' ([Errno 11001] getaddrinfo failed)\"))"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    " \n",
    "# Assume this is set in the environment:\n",
    "# export API_TOKEN=\"secret-token-123\"\n",
    " \n",
    "api_token = os.getenv(\"API_TOKEN\")\n",
    "api_url = \"https://api.cloudservice.com/v1/data\"\n",
    " \n",
    "headers = {\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_token}\"\n",
    "}\n",
    " \n",
    "try:\n",
    "    response = requests.get(api_url, headers=headers, timeout=5)\n",
    "    response.raise_for_status()\n",
    "    # Assume the API returns: {\"data\": [\"item1\", \"item2\"]}\n",
    "    items = response.json()[\"data\"]\n",
    "    print(f\"Successfully retrieved {len(items)} items.\")\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    if e.response.status_code == 401:\n",
    "        print(\"Authentication failed: Invalid token.\")\n",
    "    else:\n",
    "        print(f\"An HTTP error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ec4e1c",
   "metadata": {},
   "source": [
    "Authentication failed: Invalid token.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. The server returns a 401 status. response.raise_for_status() raises an HTTPError. The except block catches it. Inside the except block, e.response.status_code is 401, so the if condition is met, and the specific \"Authentication failed\" message is printed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc6e049",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "A developer is trying to authenticate with an API using Basic Authentication. The script sends the request, but the server responds with a 401 Unauthorized error. What is the bug in this authentication attempt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e89ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    " \n",
    "username = \"devops_user\"\n",
    "password = \"a_very_secure_password\"\n",
    "url = \"https://api.service.com/v1/status\"\n",
    " \n",
    "headers = {\n",
    "    \"username\": username,\n",
    "    \"password\": password\n",
    "}\n",
    " \n",
    "response = requests.get(url, headers=headers)\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7771ab69",
   "metadata": {},
   "source": [
    "\n",
    "Basic Authentication requires sending credentials as a tuple to the auth parameter, not as custom headers. The correct call is requests.get(url, auth=(username, password)).\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the fundamental bug. requests has a specific parameter, auth, for handling Basic Authentication. It automatically formats the username and password into the required Authorization: Basic header. Sending them as custom headers username and password is incorrect, and the server will not recognize them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e172618",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "A script needs to connect to a server that is known to be slow to respond. A timeout is configured to prevent the script from hanging. What will be the output of this script?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "504a4823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making request...\n",
      "Request successful.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    " \n",
    "# This endpoint simulates a 5-second delay before responding.\n",
    "slow_url = \"https://httpbin.org/delay/5\"\n",
    " \n",
    "try:\n",
    "    print(\"Making request...\")\n",
    "    response = requests.get(slow_url, timeout=(1.0, 3.0))\n",
    "    print(\"Request successful.\")\n",
    "except requests.exceptions.ConnectTimeout:\n",
    "    print(\"Connection timed out.\")\n",
    "except requests.exceptions.ReadTimeout:\n",
    "    print(\"Read timed out.\")\n",
    "except requests.exceptions.HTTPError:\n",
    "    print(\"An HTTP error occurred.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437e2dd8",
   "metadata": {},
   "source": [
    "Making request...\n",
    "Read timed out.\n",
    "Correct\n",
    "\n",
    "Correct. The connection is established successfully (likely in < 1.0s). The client then waits for the server's response. The server intentionally waits for 5 seconds. Since this is longer than the configured read timeout of 3.0 seconds, the requests library will raise a ReadTimeout exception, which is caught by the corresponding except block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c375e4c3",
   "metadata": {},
   "source": [
    "Question 5:\n",
    "A developer needs to check an API endpoint and print an error message if the response is a server error (5xx), but the script continues the execution of the try block and prints Status: 503 on the screen. Why doesn't the script enter the exception clause?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde251e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    " \n",
    "api_url = \"https://httpbin.org/status/503\"\n",
    " \n",
    "try:\n",
    "    response = requests.get(api_url)\n",
    "    print(f\"Status: {response.status_code}\")\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(f\"Server error detected: {e.response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e405dc",
   "metadata": {},
   "source": [
    "\n",
    "The requests.get() call itself does not raise an HTTPError on a bad status code. The developer must explicitly call response.raise_for_status() to trigger the exception.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the bug. By default, requests.get() will happily return a response object for any valid HTTP response, including 4xx and 5xx errors. To turn a bad status code into an exception, you must call response.raise_for_status(). Without this call, the try block completes successfully, and the except block is never entered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678ca642",
   "metadata": {},
   "source": [
    "Question 6:\n",
    "In what scenario is it appropriate to use a tuple for the timeout parameter, like timeout=(3.05, 27), instead of a single float?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26de5c3",
   "metadata": {},
   "source": [
    "When you need to separately control the connection timeout (time to establish a connection) and the read timeout (time to wait for data after connecting).\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. The tuple (connect, read) allows for fine-grained control. This is useful if you expect a server to be quick to connect to, but potentially slow to process and return a large response. For example, timeout=(3.05, 60) allows a quick connection check but gives the server a full minute to generate and send its response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0963b7f",
   "metadata": {},
   "source": [
    "## Advanced Resilience: API Retry Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d33cd9",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "A script makes API calls to a third-party service that occasionally fails with a 503 Service Unavailable error during peak hours. Why is it a good practice to implement a retry mechanism for this specific type of error?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd919ea",
   "metadata": {},
   "source": [
    "\n",
    "Server-side errors like 503 are often transient. A retry loop with a short delay gives the server a chance to recover, making the script more resilient to temporary glitches.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the core principle of retrying 5xx errors. The problem is likely temporary and on the server's side. Instead of failing immediately, the client can wait a moment and try again, often succeeding on the second or third attempt once the transient issue (like a brief network partition or a service restart) is resolved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dbe3d8",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "Which of the following operations is generally not idempotent and should be handled with extreme care when implementing a retry mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d00411",
   "metadata": {},
   "source": [
    "\n",
    "A POST request to /api/transactions with the body {\"amount\": 100, \"description\": \"Payment\"} to append a new transaction.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is a classic non-idempotent operation. If this request is retried due to a temporary network error after the server has already processed it, a duplicate transaction will be created. Such operations require special handling, like using a unique idempotency key in the request header, to prevent duplicate actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb4458",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "A developer implements an exponential backoff strategy, but they notice the delay between retries is not increasing as expected. What is the flaw in this exponential backoff implementation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272614a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    " \n",
    "max_retries = 4\n",
    "delay = 1 # Initial delay in seconds\n",
    " \n",
    "for attempt in range(max_retries):\n",
    "    print(f\"Attempt {attempt + 1}. Waiting for {delay}s.\")\n",
    "    time.sleep(delay)\n",
    "    \n",
    "print(\"Process complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5534444a",
   "metadata": {},
   "source": [
    "\n",
    "The delay variable is not updated inside the loop. It remains 1 for every iteration, resulting in a fixed delay, not an exponential one.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the bug. For an exponential backoff, the delay variable must be increased after each failed attempt (e.g., delay = delay * 2). Since this line is missing, the script sleeps for the same 1-second interval every time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f2b983",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "When implementing a retry strategy with exponential backoff, what is the primary purpose of adding jitter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d658f368",
   "metadata": {},
   "source": [
    "Jitter adds a small, random amount of time to the delay. This prevents multiple clients that failed at the same time from all retrying in perfect synchronization, which could overload the server again.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the exact purpose of jitter. Without it, if 100 clients fail simultaneously and all have the same exponential backoff logic, they will all retry after 1 second, then 2 seconds, then 4 seconds, etc., creating synchronized waves of traffic. Jitter spreads these retries out over a small time window, smoothing the load on the recovering server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd38a0",
   "metadata": {},
   "source": [
    "Question 5:\n",
    "A developer writes a retry loop for an API call. When the script runs against a URL that consistently returns a 400 Bad Request error, they notice it pointlessly retries three times before stopping. What is the primary logical flaw in this retry strategy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611da83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    " \n",
    "# This URL will consistently return a 400 error\n",
    "error_url = \"https://httpbin.org/status/400\"\n",
    "max_retries = 3\n",
    " \n",
    "for attempt in range(max_retries):\n",
    "    print(f\"Attempt {attempt + 1}...\")\n",
    "    try:\n",
    "        response = requests.get(error_url, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        print(\"Success!\")\n",
    "        break\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"Request failed: {e.response.status_code}. Retrying...\")\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77151374",
   "metadata": {},
   "source": [
    "\n",
    "The script retries on a 400 Bad Request error. Client errors (4xx) indicate a problem with the request itself, and retrying the same invalid request is pointless.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the logical bug. The retry logic should only be triggered for transient server-side errors (5xx) or network exceptions. A 400 Bad Request is a client error, meaning the request is flawed. The script should fail immediately and report the error instead of wasting time retrying. A proper implementation would check e.response.status_code and only retry if it's in the 500-599 range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fab641b",
   "metadata": {},
   "source": [
    "## Python Type Hints: Foundations and Basic Collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d8c948",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "What is the primary benefit of adding type hints (for example, name: str, -> int) to a Python script that will be analyzed by a static type checker like mypy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f8af5c",
   "metadata": {},
   "source": [
    "\n",
    "They allow for the detection of type-related errors before the script is run, catching potential bugs early in the development cycle.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the core purpose of static type checking. A tool like mypy can analyze the code and report errors, such as passing a list to a function that expects a str, without ever executing the code. This prevents certain classes of bugs from reaching production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4010920",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "A developer writes a function to process a list of numerical job IDs. When they run a static type checker like mypy, it reports an error for the function call. What is the cause of the static type checking error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f10ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_job_ids(ids: list[int]) -> None:\n",
    "    print(f\"Processing {len(ids)} job IDs.\")\n",
    " \n",
    "job_names: list[str] = [\"job-72\", \"job-91\"]\n",
    "process_job_ids(job_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53812d8a",
   "metadata": {},
   "source": [
    "The function is called with job_names (a list[str]), but it is defined to accept ids of type list[int].\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is a direct type mismatch. The function signature explicitly states it requires a list of integers. The static checker sees that a list of strings is being passed instead and flags this as an error, preventing a potential runtime bug if the function were to perform mathematical operations on the IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48d9050",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "A function is designed to take an optional configuration dictionary. Analyze the following code. What will be printed to the console?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67479b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User set to admin\n",
      "No settings provided.\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    " \n",
    "def apply_config(settings: Optional[dict[str, str]]) -> str:\n",
    "    if settings:\n",
    "        user = settings.get(\"USER\", \"default\")\n",
    "        return f\"User set to {user}\"\n",
    "    \n",
    "    return \"No settings provided.\"\n",
    " \n",
    "# Scenario 1\n",
    "config_data = {\"USER\": \"admin\", \"HOST\": \"prod.server\"}\n",
    "print(apply_config(config_data))\n",
    " \n",
    "# Scenario 2\n",
    "print(apply_config(None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cded49",
   "metadata": {},
   "source": [
    "Correct. In Scenario 1, settings is a dictionary, so the if settings: block is executed. settings.get(\"USER\", \"default\") finds the key and returns \"admin\". In Scenario 2, settings is None, so the if settings: condition is false, and the function proceeds to return \"No settings provided.\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a672083",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "A developer writes a function that should accept either a single port number (int) or a list of port numbers (list[int]). The static type checker flags an error on the return statement. What is the cause of the type error on the return statement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d497185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_ports(ports: int | list[int]) -> str:\n",
    "    if isinstance(ports, int):\n",
    "        return f\"Port: {ports}\"\n",
    "    else:\n",
    "        return f\"Ports: {','.join(ports)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f86228f",
   "metadata": {},
   "source": [
    "\n",
    "The str.join() method expects an iterable of strings, but mypy knows that ports is a list[int], so the elements are not strings.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. This is the precise error. str.join() requires all elements in the iterable to be strings. The static checker sees that the ports list contains integers and flags this incompatibility. The fix is to convert each integer to a string, for example, using a generator expression: ','.join(str(p) for p in ports)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaa0667",
   "metadata": {},
   "source": [
    "Question 5:\n",
    "You have a function that processes a user ID, which can be either an integer or a string representation of an integer. What is printed to the console?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cd4eea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'> <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "def get_user_id(raw_id: str | int) -> int:\n",
    "    if isinstance(raw_id, str):\n",
    "        return int(raw_id)\n",
    "    return raw_id\n",
    " \n",
    "result1 = get_user_id(123)\n",
    "result2 = get_user_id(\"456\")\n",
    " \n",
    "print(f\"{type(result1)} {type(result2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b1a438",
   "metadata": {},
   "source": [
    "Correct. For result1, get_user_id(123) is called. isinstance(123, str) is false, so the function returns the integer 123 directly. type(result1) is . For result2, get_user_id(\"456\") is called. isinstance(\"456\", str) is true, so the function returns int(\"456\"), which is the integer 456. type(result2) is ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7c7fc5",
   "metadata": {},
   "source": [
    "Question 6:\n",
    "A function is defined as def get_item(key: str) -> Optional[str]:. Why is this Optional[str] annotation more precise and useful than -> str | None?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9759bbcd",
   "metadata": {},
   "source": [
    "\n",
    "There is no difference. Optional[T] is simply an alias for Union[T, None]. The choice is purely stylistic.\n",
    "\n",
    "Correct\n",
    "\n",
    "Correct. Optional[T] is exactly equivalent to Union[T, None] (or T | None in Python 3.10+). It was created as a common shorthand to more clearly express the intent that a value can be \"present\" or \"absent\" (None). While the choice is stylistic, Optional is often preferred for its clarity in this specific use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
